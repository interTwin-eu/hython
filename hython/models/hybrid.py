import torch
from torch import nn


class Hybrid(nn.Module):
    def __init__(
        self,
        transfernn,
        head_layer,
        freeze_head=True,
        scale_head_input_parameter=True,
        scale_head_output=False,
    ):
        super(Hybrid, self).__init__()

        self.transfernn = transfernn
        self.head_layer = head_layer
        self.scale_head_input_parameter = scale_head_input_parameter
        self.scale_head_output = scale_head_output

        # freeze weights
        if freeze_head:
            for weight in self.head_layer.parameters():
                weight.requires_grad = False

    def forward(self, x_transf, x_head):
        """
        Parameters
        ----------
        x_transf: torch.Tensor
            Tensor of size [batch_size, n_predictor] (N, C) or [batch_size, seq_length, n_predictor] (N, T, C)
        x_head: torch.Tensor
            Tensor of size [batch_size, seq_length, n_param] (N, T, C)
        """
        # run trasnferNN
        param = self.transfernn(x_transf)  # output: N T C or N C

        if self.scale_head_input_parameter:
            # the effective parameters (ep) generated by the transfernn should
            # be in the range expected by the head layer. For the calibration
            # the head layer (surrogate) expects ep in the 0-1 range, therefore the
            # rescale_input method applies a sigmoid function to the output of the
            # transfernn
            param = self.head_layer.rescale_input(param)  # output: N T C or N C

        # concat to x_head, as of now add time dimension ot static params
        x_head_concat = torch.concat(
            [
                x_head,
                param.unsqueeze(1).repeat(1, x_head.size(1), 1),
            ],
            dim=2,
        )

        # run head layer
        output = self.head_layer(x_head_concat)["y_hat"]

        if self.scale_head_output:
            output = output

        return {"y_hat": output, "param": param}
