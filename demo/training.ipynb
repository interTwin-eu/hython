{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "913afcd2-a55e-4804-adf4-699a3f3a660a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a0ce1-163e-40ac-9e7e-e6d5f364bd84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4635e3f-8a34-48ac-b4f5-d871bdfcda0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "import dask\n",
    "from torch import nn\n",
    "\n",
    "from hython.preprocess import apply_normalization\n",
    "from hython.datasets.datasets import LSTMDataset\n",
    "from hython.train_val import train_val\n",
    "from hython.sampler import RegularIntervalSampler, DataLoaderSpatialSampler\n",
    "from hython.metrics import mse_metric\n",
    "from hython.losses import RMSELoss\n",
    "from hython.utils import read_from_zarr, missing_location_idx, get_sampler_config, set_seed\n",
    "from hython.models.lstm import CustomLSTM\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "from hython.viz import plot_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a78cf5-bd72-4a3d-bd18-ce49d0e8fdee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3186025-caa2-429f-88da-8c9c6b23c292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WFLOW_MODEL =  \"datademo\"\n",
    "SURROGATE_MODEL = \"demo\"\n",
    "EXP = \"demo\" # experiment name\n",
    "SEED = 1696\n",
    "\n",
    "dynamic_names = [\"precip\", \"pet\", \"temp\"] \n",
    "static_names = [ 'thetaS', 'thetaR', 'RootingDepth', 'Swood','KsatVer', \"Sl\"] \n",
    "target_names = [ \"vwc\",\"actevap\"] \n",
    "\n",
    "# DL model hyper parameters\n",
    "HIDDEN_SIZE = 24\n",
    "INPUT_SIZE = len(dynamic_names)\n",
    "OUTPUT_SIZE = len(target_names)\n",
    "NUMBER_STATIC_PREDICTORS = len(static_names)\n",
    "TARGET_WEIGHTS = {t:0.5 for t in target_names}\n",
    "\n",
    "# train/val parameters\n",
    "\n",
    "train_start = \"2016-01-01\"\n",
    "train_end = \"2018-12-31\"\n",
    "train_range = slice(train_start,train_end)\n",
    "\n",
    "EPOCHS = 20\n",
    "SPATIAL_BATCH_SIZE = 256\n",
    "TEMPORAL_SAMPLING_SIZE = 150 \n",
    "SEQ_LENGTH = 360 \n",
    "\n",
    "# sampler parameters\n",
    "\n",
    "INTERVALS = [4, 4] # km\n",
    "TRAIN_ORIGIN = [0, 0]\n",
    "VAL_ORIGIN = [2, 2]\n",
    "\n",
    "# missing policy\n",
    "remove_lakes = True\n",
    "\n",
    "assert sum(v for v in TARGET_WEIGHTS.values()) == 1, \"check target weights\"\n",
    "TARGET_INITIALS = \"\".join([i[0].capitalize() for i in target_names])\n",
    "\n",
    "# paths\n",
    "wd =  Path(\"../data/datademo\")\n",
    "surrogate_data = wd / f\"{WFLOW_MODEL}.zarr\"\n",
    "dp_surrogate_model = wd / SURROGATE_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c67bf4f-55e3-46fc-927b-72711d831f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b352df1a-dd0d-4c51-bc56-99b55bd697d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read from preprocessed data, select train range\n",
    "\n",
    "# train\n",
    "Xd = read_from_zarr(url=surrogate_data, group=\"xd\", multi_index=\"gridcell\").sel(time = train_range).xd\n",
    "Xs = read_from_zarr(url=surrogate_data, group=\"xs\", multi_index=\"gridcell\").xs\n",
    "Y = read_from_zarr(url=surrogate_data, group=\"y\", multi_index=\"gridcell\").sel(time = train_range).y\n",
    "\n",
    "# other \n",
    "# wflow_lakes = Xs.sel(feat=\"wflow_lakeareas\").unstack()\n",
    "#wflow_dem = Xs.sel(feat=\"wflow_dem\").unstack()\n",
    "\n",
    "# select features and targets \n",
    "Xd = Xd.sel(feat=dynamic_names)\n",
    "Xs = Xs.sel(feat=static_names)\n",
    "Y = Y.sel(feat=target_names)\n",
    "\n",
    "print(Xd.shape, Xs.shape, Y.shape)\n",
    "# read masks\n",
    "mask_missing = read_from_zarr(url=surrogate_data, group=\"mask\" ).mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e457ac4-2554-4eb8-bdb5-a65a85cf4556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the spatial samplers for both training and validation sets. Remeber the subsets should not overlap, so choose different origins.\n",
    "\n",
    "# training set\n",
    "spatial_train_sampler = RegularIntervalSampler(intervals = INTERVALS, origin = TRAIN_ORIGIN)\n",
    "\n",
    "# validation set\n",
    "spatial_val_sampler = RegularIntervalSampler(intervals = INTERVALS, origin = VAL_ORIGIN) \n",
    "\n",
    "# Apply the samplers: return the cell indices that can be used later in training and validation to sample the whole spatial domain.\n",
    "data2d  = mask_missing.values\n",
    "\n",
    "idx = missing_location_idx(Xs.values)\n",
    "\n",
    "sampler_train_meta = spatial_train_sampler.sampling_idx(data2d, mask_missing)\n",
    "sampler_val_meta = spatial_val_sampler.sampling_idx(data2d, mask_missing)\n",
    "\n",
    "# some useful metadata\n",
    "print(sampler_train_meta)\n",
    "\n",
    "# check location of training and validation sets\n",
    "#_ = plot_sampler(mask_missing, sampler_train_meta, sampler_val_meta, figsize= (8, 8 ), markersize = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e88b5b-4249-42ad-9dd6-5820c7dd4247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Percentage of sampled data: \",(len(sampler_train_meta.idx_sampled_1d_nomissing)/ len(Xs[~idx].values.flatten()))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3269824-2648-4062-badd-f594fe090334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Normalizing\n",
    "\n",
    "# statistics from training set\n",
    "_, d_m, d_std = apply_normalization(Xd[sampler_train_meta.idx_sampled_1d_nomissing], type = \"spacetime\", how ='standard')\n",
    "_, s_m, s_std = apply_normalization(Xs[sampler_train_meta.idx_sampled_1d_nomissing], type = \"space\", how ='standard')\n",
    "_, y_m, y_std = apply_normalization(Y[sampler_train_meta.idx_sampled_1d_nomissing], type = \"spacetime\", how ='standard')\n",
    "\n",
    "# normalize training set and validation set\n",
    "Xd = apply_normalization(Xd, type=\"spacetime\", how=\"standard\", m1 = d_m, m2 = d_std).compute()\n",
    "Xs = apply_normalization(Xs, type=\"space\", how=\"standard\",  m1 = s_m, m2 = s_std).compute()\n",
    "Y = apply_normalization(Y, type=\"spacetime\",how=\"standard\", m1 = y_m, m2 = y_std).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a747bc5a-fe58-4c63-8945-55e660dd9208",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare Model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fecaecc-4aa4-409a-9031-800d694f92c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "Xs = torch.Tensor(Xs.values)\n",
    "Xd = torch.Tensor(Xd.values)\n",
    "Y = torch.Tensor(Y.values)\n",
    "\n",
    "print(Xs.shape, Xd.shape, Y.shape)\n",
    "\n",
    "# init datasets\n",
    "dataset = LSTMDataset(Xd, Y, Xs)\n",
    "\n",
    "train_sampler = DataLoaderSpatialSampler(dataset, num_samples=1, sampling_indices = sampler_train_meta.idx_sampled_1d_nomissing.tolist())\n",
    "valid_sampler = DataLoaderSpatialSampler(dataset, num_samples=1, sampling_indices = sampler_val_meta.idx_sampled_1d_nomissing.tolist())\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=SPATIAL_BATCH_SIZE, shuffle=False, sampler = train_sampler) # implement shuffling in the sampler!\n",
    "val_loader = DataLoader(dataset, batch_size=SPATIAL_BATCH_SIZE, shuffle=False, sampler = valid_sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaedbed-af8b-44da-a677-a56bed8b8706",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b046cecf-ee3c-4b85-b68b-bfd62089e87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = CustomLSTM(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, NUMBER_STATIC_PREDICTORS)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0751f35d-23e0-4dcf-b67b-b034964f2a87",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train/valid settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ad206-3e94-4624-8924-fee31dbed23a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path2models= \"./checkpoints\" \n",
    "if not os.path.exists(path2models):\n",
    "    os.mkdir(path2models)\n",
    "    \n",
    "    \n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "loss_fn = RMSELoss(target_weight=TARGET_WEIGHTS)\n",
    "\n",
    "\n",
    "metric_fn = mse_metric\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f0048-e9d9-4819-848f-d4c96e487810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_train={\n",
    "    \"num_epochs\": EPOCHS,\n",
    "    \"temporal_sampling_idx_change_with_epoch\": True,\n",
    "    \"temporal_sampling_size\": TEMPORAL_SAMPLING_SIZE,\n",
    "    \"seq_length\": SEQ_LENGTH,\n",
    "    \"ts_range\": Y.shape[1],\n",
    "    \"optimizer\": opt,\n",
    "    \"loss_func\": loss_fn,\n",
    "    \"metric_func\": metric_fn,\n",
    "    \"train_dl\": train_loader, \n",
    "    \"val_dl\": val_loader,\n",
    "    \"lr_scheduler\": lr_scheduler,\n",
    "    \"path2weights\": f\"{path2models}/weights.pt\",\n",
    "    \"device\":device,\n",
    "    \"target_names\": target_names\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aae90c-39ff-42ef-bbaa-77e04da01f40",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Train/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36e585-beff-41d4-96b6-e02027b74348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, sm_loss_history, sm_metric_history = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185ef42-f8b0-4841-ad50-6bdbb25b27ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lepochs = list(range(1,params_train[\"num_epochs\"] + 1))\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize= (12,6), sharex=True)\n",
    "\n",
    "axs[0].plot(lepochs, sm_metric_history['train_vwc'], marker='.', linestyle='-', color='b', label='Training')\n",
    "axs[0].plot(lepochs, sm_metric_history['val_vwc'], marker='.', linestyle='-', color='r', label='Validation')\n",
    "#axs[0].title('Validation Loss - SM')\n",
    "axs[0].set_ylabel(metric_fn.__name__)\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "axs[1].plot(lepochs, sm_metric_history['train_actevap'], marker='.', linestyle='-', color='b', label='Training')\n",
    "axs[1].plot(lepochs, sm_metric_history['val_actevap'], marker='.', linestyle='-', color='r', label='Validation')\n",
    "#axs[0].title('Validation Loss - SM')\n",
    "axs[1].set_ylabel(metric_fn.__name__)\n",
    "axs[1].grid(True)\n",
    "\n",
    "axs[2].plot(lepochs, [i.detach().cpu().numpy() for i in sm_loss_history['train']], marker='.', linestyle='-', color='b', label='Training')\n",
    "axs[2].plot(lepochs, [i.detach().cpu().numpy() for i in sm_loss_history['val']], marker='.', linestyle='-', color='r', label='Validation')\n",
    "#axs[0].title('Validation Loss - SM')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel(loss_fn.__name__)\n",
    "axs[2].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71c778-9b42-4cda-98c7-5dd075bb3b02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "fp = wd / f\"{SURROGATE_MODEL}_{EXP}_v{TARGET_INITIALS}_h{HIDDEN_SIZE}_s{SEED}.pt\"\n",
    "torch.save(model.state_dict(), fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emulator",
   "language": "python",
   "name": "emulator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
