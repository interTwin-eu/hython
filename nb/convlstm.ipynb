{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49640666-0cfe-4973-b629-8ef88626bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f646582a-3f9c-4e27-bd5b-f24cd80c6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from hython.utils import write_to_zarr, build_mask_dataarray\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from hython.datasets.datasets import get_dataset\n",
    "from numcodecs import Blosc\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7990a6-4e87-4465-a9bd-c10b11258c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hython.trainer import XBatcherTrainer\n",
    "from hython.trainer import train_val\n",
    "from hython.sampler import SamplerBuilder\n",
    "from hython.metrics import MSEMetric\n",
    "from hython.losses import RMSELoss\n",
    "from hython.utils import read_from_zarr, set_seed\n",
    "from hython.models.cudnnLSTM import CuDNNLSTM\n",
    "from hython.trainer import RNNTrainer, RNNTrainParams\n",
    "from hython.normalizer import Normalizer\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d7e02-8ab4-497e-9277-460a5d10183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchvision.datasets import MovingMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb0b9f0-ce33-4d60-ac54-41d4143e2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_surr_input = Path(\"/mnt/CEPH_PROJECTS/InterTwin/Wflow/models/adg1km_eobs\")\n",
    "\n",
    "\n",
    "static = xr.open_dataset(dir_surr_input / \"staticmaps.nc\")#.chunk(\"auto\")\n",
    "dynamic = xr.open_dataset(dir_surr_input/ \"forcings.nc\").chunk(\"auto\") # C T W H => N T C H W\n",
    "target = xr.open_dataset(dir_surr_input / \"run_default/output.nc\").sel(layer=1).isel(lat=slice(None, None, -1))#.chunk(\"auto\") # C T W H => N T C H W\n",
    "\n",
    "\n",
    "surr_model_output = \"convlstm.pt\"\n",
    "experiment = \"exp1\" # experiment name\n",
    "\n",
    "dir_surr_output = \"/mnt/CEPH_PROJECTS/InterTwin/hydrologic_data/surrogate_model\"\n",
    "\n",
    "SEED = 1696\n",
    "\n",
    "dynamic_names = [\"precip\", \"pet\", \"temp\"] \n",
    "static_names = [ 'thetaS', 'thetaR', 'RootingDepth', 'Swood','KsatVer', \"Sl\"] \n",
    "target_names = [ \"vwc\",\"actevap\"] \n",
    "\n",
    "mask_from_static = [\"wflow_lakeareas\"]\n",
    "rename_mask = [\"mask_lake\"]\n",
    "\n",
    "\n",
    "dataset = \"XBatchDataset\"\n",
    "\n",
    "\n",
    "# DL model hyper parameters\n",
    "HIDDEN_SIZE = 24\n",
    "DYNAMIC_INPUT_SIZE = len(dynamic_names)\n",
    "STATIC_INPUT_SIZE = len(static_names)\n",
    "KERNEL_SIZE = (3, 3)\n",
    "NUM_LSTM_LAYER = 2\n",
    "OUTPUT_SIZE = len(target_names)\n",
    "\n",
    "\n",
    "TARGET_WEIGHTS = {t:0.5 for t in target_names}\n",
    "\n",
    "\n",
    "\n",
    "# train/test parameters\n",
    "train_temporal_range = slice(\"2016-01-01\",\"2018-12-31\")\n",
    "test_temporal_range = slice(\"2019-01-01\", \"2020-12-31\")\n",
    "\n",
    "EPOCHS = 90\n",
    "BATCH = 64\n",
    "TEMPORAL_SUBSAMPLING = True\n",
    "TEMPORAL_SUBSET = [150, 150] \n",
    "SEQ_LENGTH = 360\n",
    "\n",
    "\n",
    "assert sum(v for v in TARGET_WEIGHTS.values()) == 1, \"check target weights\"\n",
    "TARGET_INITIALS = \"\".join([i[0].capitalize() for i in target_names])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c62cf-d370-4867-8388-50b06d107040",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = Path(\"/mnt/CEPH_PROJECTS/InterTwin/Wflow/models/adg1km_eobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad3f08-c6b0-48b9-a062-bd5e6923f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dynamic = dynamic.rename({\"latitude\":\"lat\", \"longitude\":\"lon\"})\n",
    "    static = static.rename({\"latitude\":\"lat\", \"longitude\":\"lon\"})\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468a748-9c53-4653-a4bc-05919e3e50a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking \n",
    "\n",
    "mask_missing = np.isnan(static[static_names[0]]).rename(\"mask_missing\")\n",
    "\n",
    "masks = []\n",
    "masks.append(mask_missing)\n",
    "\n",
    "for i, mask in enumerate(mask_from_static):\n",
    "    masks.append((static[mask] > 0).astype(np.bool_).rename(rename_mask[i]))\n",
    "\n",
    "masks = build_mask_dataarray(masks, names = [\"mask_missing\"]+ rename_mask).any(dim=\"mask_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4cfe2-6156-4aa9-8884-a3999e956640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter \n",
    "dynamic = dynamic[dynamic_names]\n",
    "target = target[target_names]\n",
    "static = static.drop_dims(\"time\").sel(layer=1)[ static_names ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb95d0-719a-4ca7-be61-166021195b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand static to dynamic \n",
    "time_da = xr.DataArray(dynamic.time.values, [('time', dynamic.time.values)])\n",
    "static = static.expand_dims({\"time\":time_da})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615dd56-1e2e-4e84-9d1d-45bc965746fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic = dynamic.to_array() # C T H W\n",
    "static = static.to_array() # C T H W\n",
    "target = target.to_array() # C T H W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643e80e-0ff8-4b95-acea-f91a1d6b6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_train = dynamic.sel(time=train_temporal_range)\n",
    "static_train = static.sel(time=train_temporal_range)\n",
    "target_train = target.sel(time=train_temporal_range)\n",
    "\n",
    "dynamic_test = dynamic.sel(time=test_temporal_range)\n",
    "static_test = static.sel(time=test_temporal_range)\n",
    "target_test = target.sel(time=test_temporal_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664195f7-3bfd-4418-9998-fc9ab9e0fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train.shape, static_train.shape, dynamic_train.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e42b0-5209-4c5c-8a03-e7e0267aae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "\n",
    "normalizer_dynamic = Normalizer(method=\"standardize\", type=\"spacetime\", shape=\"2D\")\n",
    "\n",
    "normalizer_static = Normalizer(method=\"standardize\", type=\"space\", shape=\"2D\")\n",
    "\n",
    "normalizer_target = Normalizer(method=\"standardize\", type=\"spacetime\", shape=\"2D\")\n",
    "\n",
    "normalizer_dynamic.compute_stats(dynamic_train)\n",
    "normalizer_static.compute_stats(static_train)\n",
    "normalizer_target.compute_stats(target_train)\n",
    "\n",
    "# TODO: save stats, implement caching of stats to save computation\n",
    "\n",
    "dynamic = normalizer_dynamic.normalize(dynamic_train)\n",
    "static = normalizer_static.normalize(static_train)\n",
    "target = normalizer_target.normalize(target_train)\n",
    "\n",
    "dynamic_test = normalizer_dynamic.normalize(dynamic_test)\n",
    "static_test = normalizer_static.normalize(static_test)\n",
    "target_test = normalizer_target.normalize(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb556bd-0c95-495d-9602-96b8f011ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking \n",
    "dynamic_train = dynamic_train.where(~masks, 0)\n",
    "\n",
    "static_train = static_train.where(~masks, 0)\n",
    "\n",
    "target_train = target_train.where(~masks, 0)\n",
    "\n",
    "\n",
    "dynamic_test = dynamic_test.where(~masks, 0)\n",
    "\n",
    "static_test = static_test.where(~masks, 0)\n",
    "\n",
    "target_test = target_test.where(~masks, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe79b9-57a1-417c-a048-a5ec49437385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressor = Blosc(cname='zl4', clevel=9, shuffle=Blosc.BITSHUFFLE)\n",
    "\n",
    "# ss = params.drop_dims(\"time\")[[ 'thetaS', 'thetaR', 'RootingDepth', 'Swood','KsatVer', \"Sl\"]].expand_dims({\"time\": ds.time}).chunk({\"time\":500, \"latitude\":50, \"longitude\":50})\n",
    "\n",
    "# ss.to_zarr(wd / \"test.zarr\",storage_options={\"compressor\":compressor})\n",
    "\n",
    "# ss = xr.open_dataset( wd / \"test.zarr\", engine = \"zarr\")\n",
    "\n",
    "# time, lat, lon = 365, 16, 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b85746-63b6-4233-9927-0fd6e928ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xbatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe50f9f-90e9-44a7-9f2a-3dcdaab6d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbatcher.BatchGenerator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e8a00-733e-4c1c-b1ab-9b7e43cebd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83f82d0a-ed80-4ba9-a948-ed84edf1a9f8",
   "metadata": {},
   "source": [
    "## Test xbatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d5d48-3c11-4cb8-ac78-b584cbce799a",
   "metadata": {},
   "source": [
    "Check whether I can get and index of the chunks so that I can do:\n",
    "\n",
    "$$ \\mathbf{f}: \\mathbb{R}^1 \\rightarrow \\mathbb{R}^2$$\n",
    "\n",
    "f(0) => (0,0) </br>\n",
    "f(1) => (0,1) </br>\n",
    "f(2) => (1,0) </br>\n",
    "f(3) => (1,1) </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7571fc6-e96e-435f-86f2-a5a54fed2322",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgen = xbatcher.BatchGenerator(\n",
    "    dynamic_train,\n",
    "    input_dims={\"lat\":80, \"lon\":80, \"time\":360}, # dimension size of the sample cube\n",
    "    preload_batch=True,\n",
    "    #batch_dims={\"time\":60, \"lat\":80, \"lon\":80},\n",
    "    #concat_input_dims= True,\n",
    "    #input_overlap={\"time\":10, \"lat\":10, \"lon\":10} # overlaps between dimensions of each cube\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3682c-6c62-4810-94b1-3084c1e7b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203fc629-3f28-4eb7-b2f2-e2018d1ed260",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = xgen[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bea281-de30-4551-9489-4590d05f3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.merge(xgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37d573-1eb1-424c-84f1-23a15dc8ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(time=199).precip.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2db81-8e39-4a7c-85a0-e85682a16cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should work for the convLSTM \n",
    "sample_convlstm = sample.to_stacked_array(new_dim=\"feat\", sample_dims=(\"lat\",\"lon\", \"time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565f11f-ddf2-446f-a356-7f3f0c81170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_convlstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98dbd57-bc4b-448d-b3cc-2a283a31cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to handle null cubes\n",
    "sample_convlstm.isnull().all().item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3affaf6f-be34-4101-8557-d5a1d5ecd3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_convlstm.sel(feat=\"precip\").isel(time=10).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e883dc3-599a-4a13-9511-aa70bce17076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should work for the 1D lstm\n",
    "sample_lstm = sample_convlstm.stack(gridcell=[\"lat\",\"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cebd7c-e534-42af-808f-1c68f549f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds.sel(variable=\"precip\").to_dataset(name=\"ds\").to_stacked_array(new_dim=\"batch\", sample_dims=(\"lon\",\"lat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc979c13-d9a5-4f6b-9926-bd3238605d10",
   "metadata": {},
   "source": [
    "## Example xbatcher from https://github.com/earth-mover/dataloader-demo/blob/main/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d15c09-beec-4331-b9fa-b3a568e3eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_json(obj):\n",
    "    print(json.dumps(obj))\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import multiprocessing\n",
    "class XBatcherPyTorchDataset(TorchDataset):\n",
    "    def __init__(self, batch_generator: xbatcher.BatchGenerator):\n",
    "        self.bgen = batch_generator\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bgen)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t0 = time.time()\n",
    "        print_json(\n",
    "            {\n",
    "                \"event\": \"get-batch start\",\n",
    "                \"time\": t0,\n",
    "                \"idx\": idx,\n",
    "                \"pid\": multiprocessing.current_process().pid,\n",
    "            }\n",
    "        )\n",
    "        # load before stacking\n",
    "        batch = self.bgen[idx].load()\n",
    "\n",
    "        print(batch)\n",
    "\n",
    "        # Use to_stacked_array to stack without broadcasting,\n",
    "        stacked = batch.to_stacked_array(\n",
    "            new_dim=\"batch\", sample_dims=(\"time\", \"longitude\", \"latitude\")\n",
    "        ).transpose(\"time\", \"batch\", ...)\n",
    "        print(stacked)\n",
    "        x = torch.tensor(stacked.data)\n",
    "        t1 = time.time()\n",
    "        print_json(\n",
    "            {\n",
    "                \"event\": \"get-batch end\",\n",
    "                \"time\": t1,\n",
    "                \"idx\": idx,\n",
    "                \"pid\": multiprocessing.current_process().pid,\n",
    "                \"duration\": t1 - t0,\n",
    "            }\n",
    "        )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90560ee3-6cdb-4dc4-aaaa-47149a906cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json\n",
    "def setup(source=\"gcs\", patch_size: int = 48, input_steps: int = 3):\n",
    "    if source == \"gcs\":\n",
    "        ds = xr.open_dataset(\n",
    "            \"gs://weatherbench2/datasets/era5/1959-2022-6h-128x64_equiangular_with_poles_conservative.zarr\",\n",
    "            engine=\"zarr\",\n",
    "            chunks={},\n",
    "        )\n",
    "    elif source == \"arraylake\":\n",
    "        config.set({\"s3.endpoint_url\": \"https://storage.googleapis.com\", \"s3.anon\": True})\n",
    "        ds = (\n",
    "            Client()\n",
    "            .get_repo(\"earthmover-public/weatherbench2\")\n",
    "            .to_xarray(\n",
    "                group=\"datasets/era5/1959-2022-6h-128x64_equiangular_with_poles_conservative\",\n",
    "                chunks={},\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown source {source}\")\n",
    "\n",
    "    DEFAULT_VARS = [\n",
    "        \"10m_wind_speed\",\n",
    "        \"2m_temperature\",\n",
    "        \"specific_humidity\",\n",
    "    ]\n",
    "\n",
    "    ds = ds[DEFAULT_VARS]\n",
    "    patch = dict(\n",
    "        latitude=patch_size,\n",
    "        longitude=patch_size,\n",
    "        time=input_steps,\n",
    "    )\n",
    "    overlap = dict(latitude=32, longitude=32, time=input_steps // 3 * 2)\n",
    "\n",
    "    bgen = xbatcher.BatchGenerator(\n",
    "        ds,\n",
    "        input_dims=patch,\n",
    "        input_overlap=overlap,\n",
    "        preload_batch=False,\n",
    "    )\n",
    "\n",
    "    dataset = XBatcherPyTorchDataset(bgen)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10aa5c-1a68-489c-9ad3-c85639196ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgen = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1924aaef-4357-49a3-ab13-a304fbd468b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea21f4-8df8-4b1b-9a72-0d0b9233e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xgen[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abba2d3-caba-4326-9008-292c49d6cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res[1]\n",
    "res.shape, res.dims, res.coords, len(xgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb573312-a85b-4ce6-9e92-4ba5deed46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, b in enumerate(xgen):\n",
    "#     print(i)\n",
    "#     print(b.shape)\n",
    "#     plt.figure()\n",
    "#     try:\n",
    "#         b.isel(variable_input=1, sample=0).plot()\n",
    "#         b.isel(variable_input=1, lat_input=100, lon_input=100).plot(x=\"time\")\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         b.isel(variable=1, time=1).plot()\n",
    "#         b.isel(variable=1, lat=10, lon=10).plot()\n",
    "#     if i > 20:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9a05c-1662-4b26-a1d7-69bfa62d4498",
   "metadata": {},
   "source": [
    "## Test custom \"xbatcher\"\n",
    "\n",
    "The current implementation of xbatcher looks cool but it lacks:\n",
    "- way to index tiles and sequence, it only indexes cubes, therefore how I can subsample only in one of the two dimensions?\n",
    "- as I don't know the ordering of the cube samples, how can I subsample?\n",
    "- how to handle NULL cubes\n",
    "- It drops the \"edges\" of the dimension (i.e. it does not provide a collate function and cut short returning sample of the same dimension size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c4c61-cf2d-4e79-a45d-14a934de7a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hython.sampler import compute_grid_indices\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e138a51-eb65-44ea-91dd-16e482e793ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing cubelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ecb10-36b0-4e87-953e-083582fd86cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f19b0-6e9c-421f-a85a-5134b3cbda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dynamic#.transpose(\"lat\", \"lon\", \"time\") # x, y, t\n",
    "\n",
    "data.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01cd01-4ca4-4a73-a4f2-0bcb6b1b8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe33ad-a3a8-4433-bb4b-d17a069299b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e0dbe-0c6e-4bd2-847b-060bbecdac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsize, ysize, tsize = 20, 20, 360 #4748 # cubelet dimension size\n",
    "xover, yover, tover= 0, 0, 0 # cubelets overlaps\n",
    "\n",
    "\n",
    "space_idx = compute_grid_indices(grid=data)\n",
    "\n",
    "print(space_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f7bb2-c8ed-41be-8f3b-be2ab8517708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create cubelets, keep or not edge cubelets\n",
    "# keep_edge_cubelets = True\n",
    "\n",
    "# space_indices = []\n",
    "# space_indices_all_missing = []\n",
    "# space_slices = []\n",
    "# idx = 0\n",
    "# for ix,iy in zip(range(0, data.shape[0], xsize - xover), range(0, data.shape[1], ysize - yover)):\n",
    "#     xslice = slice(ix, ix + xsize)\n",
    "#     yslice = slice(iy, iy + ysize)\n",
    "#     cubelet = space_idx[xslice, yslice]\n",
    "#     mask_cubelet = masks[xslice, yslice]\n",
    "    \n",
    "#     #plt.figure(figsize=(2,2))\n",
    "#     #plt.imshow(cubelet)\n",
    "#     #plt.annotate(idx, list(map(lambda x: int(x/4), cubelet.shape)) ,color=\"red\", size=20)\n",
    "#     #plt.colorbar()\n",
    "#     space_slices.append([xslice, yslice])\n",
    "#     space_indices.append(idx)\n",
    "#     if mask_cubelet.all().item(0):\n",
    "#         space_indices_all_missing.append(idx)\n",
    "#     idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b109ae-797f-48b6-b6b7-e1f4b80025c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cubelets, keep or not edge cubelets\n",
    "\n",
    "# THIS DEPENDS ON WHICH AXES are the spatial coordinates!\n",
    "\n",
    "data_current_coordinates = {\"time\":0, \"lat\":1, \"lon\":2}\n",
    "\n",
    "data_time_size = len(data.time)\n",
    "data_lat_size = len(data.lat)\n",
    "data_lon_size = len(data.lon)\n",
    "\n",
    "keep_edge_cubelets = False\n",
    "\n",
    "space_indices = []\n",
    "space_indices_all_missing = []\n",
    "space_slices = []\n",
    "idx = 0\n",
    "for ix in range(0, data_lon_size, ysize - yover):\n",
    "    for iy in range(0, data_lat_size, xsize - xover):\n",
    "        xslice = slice(ix, ix + xsize)\n",
    "        yslice = slice(iy, iy + ysize)\n",
    "        # don't need the original data, but a derived 2D array of indices, very light! \n",
    "        cubelet = space_idx[xslice, yslice]\n",
    "\n",
    "        # decide whether keep or not degenerate cubelets, otherwise these can be restored in the dataset using the collate function, which will fill with zeros\n",
    "        if not keep_edge_cubelets:\n",
    "            if cubelet.shape != (ysize, xsize):\n",
    "                continue\n",
    "\n",
    "        space_slices.append([xslice, yslice])\n",
    "        space_indices.append(idx)\n",
    "        \n",
    "        # keep or not cubelets that are all nans\n",
    "        mask_cubelet = masks[xslice, yslice]\n",
    "        if mask_cubelet.all().item(0):\n",
    "            space_indices_all_missing.append(idx)\n",
    "            \n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4bd1a9-0fdc-48ed-9a4c-e1dac7f4c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_time_size, data_lat_size, data_lon_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00dccc-60e1-4dc8-8449-0040ac7bcd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indices = []\n",
    "time_slices = []\n",
    "idx = 0\n",
    "\n",
    "latlon = data_current_coordinates[\"lat\"], data_current_coordinates[\"lon\"]\n",
    "\n",
    "for it in range(0, data_time_size, tsize - tover):\n",
    "    tslice = slice(it, it + tsize)\n",
    "    \n",
    "    # this requires the actual dataset? probably an array of a variable\n",
    "    # probably don't need raw data\n",
    "    \n",
    "    if data_current_coordinates[\"time\"] == 0:\n",
    "        cubelet = data.precip[tslice,...]\n",
    "    elif data_current_coordinates[\"time\"] == len(data_current_coordinates.keys()):\n",
    "        cubelet = data.precip[...,tslice]\n",
    "    else:\n",
    "        cubelet = data.precip[...,tslice,...]\n",
    "        \n",
    "    if not keep_edge_cubelets:\n",
    "        if cubelet.shape[data_current_coordinates[\"time\"]] != tsize:\n",
    "            continue\n",
    "            \n",
    "    time_indices.append(idx)\n",
    "    time_slices.append(tslice)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11da6d-55f8-4ff3-8221-9bf1ebab619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e2e90-8884-47ce-b1d8-dc50ed365173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cubelets idx\n",
    "cube_idx = list(itertools.product(*(space_indices, time_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5f10e-4097-4f14-8845-b05f0ee02519",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ada596-9db2-4b9f-a9cf-6a4b5b102e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slices\n",
    "slice_idx = list(itertools.product(*(space_slices, time_slices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c67041-a1b4-4505-a64d-c2b2982c57f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slice_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5980f-7226-47ac-9e47-afb3514251fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7bfc3-7890-4ff7-b608-834e21a71dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping\n",
    "mapping_cubelets_slices = {}  # coordinates sequence should be as the model expects \n",
    "print(data_current_coordinates)\n",
    "# Actually the slicing occurs at the getitem of the dataset, so after the data is kind of transposed\n",
    "\n",
    "data.to_stacked_array( new_dim=\"feat\", sample_dims = [\"time\", \"lat\", \"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7920ef-212f-4729-b03d-694489335849",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ic, islice in zip(cube_idx, slice_idx):\n",
    "    m = {\"time\":\"\", \"lat\":\"\", \"lon\":\"\"}\n",
    "    sp_slice, t_slice = islice\n",
    "    tot_slice = (sp_slice[0], sp_slice[1], t_slice) # T C H W\n",
    "    m.update({\"time\":t_slice})\n",
    "    m.update({\"lat\":sp_slice[1]})\n",
    "    m.update({\"lon\":sp_slice[0]})\n",
    "    mapping_cubelets_slices[ic] = m # (sp_slice[0], sp_slice[1], t_slice)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74173961-4605-4b07-9d52-3d9a34ba0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_cubelets_slices[(0,0)].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4b9ac-5405-4b42-983e-e488cc18054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that maps the cubelet indices to the grid indices for chunking\n",
    "\n",
    "def return_cubelet_slices(cubelet_idx):\n",
    "    return mapping_cubelets_slices[cubelet_idx]\n",
    "\n",
    "def return_cubelet_data(data,cubelet_idx):\n",
    "    return data[*mapping_cubelets_slices[cubelet_idx].values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305d0c9-dc7c-4e11-8e24-c3c79002195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_cubelet_data(data.precip, (1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06e4c2-6f0b-4a55-ada5-5bc34acb4f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "def cubelet_idx_with_all_missing_values(mapping_cubelets, cubelets_idx_missing, time_indices):\n",
    "\n",
    "    new_map = mapping_cubelets.copy()\n",
    "    for t in time_indices:\n",
    "        for idx in cubelets_idx_missing:\n",
    "            try:\n",
    "                new_map.pop((idx,t)) \n",
    "            except:\n",
    "                pass\n",
    "    return new_map\n",
    "\n",
    "# can create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc5208-f42e-45a3-93ec-65b7f4b8be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mapping = cubelet_idx_with_all_missing_values(mapping_cubelets_slices, space_indices_all_missing, time_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1894ce-d72d-43ac-b347-a97a670377f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample space and time, this becomes a class like RandomCubeletsSampler\n",
    "\n",
    "# keys (space, time)\n",
    "new_mapping\n",
    "\n",
    "def subsample(mapping, time_indices, space_indices):\n",
    "\n",
    "    new_mapping = {}\n",
    "    for filter_key in itertools.product(space_indices, time_indices):\n",
    "        #print(filter_key)\n",
    "        value = mapping.get(filter_key, None)\n",
    "        if value is not None:\n",
    "            new_mapping[filter_key] = value\n",
    "\n",
    "    return new_mapping        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d4953-03c7-497e-8bc8-4f9c1478b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indices, space_indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5973b8-a039-4330-a48c-7ef508ab654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sub_mapping = subsample(new_mapping, [0,1,2], [i for i in space_indices if i % 2 == 0]) # only even indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d361624-4421-48e3-9688-6b2932ba56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(new_sub_mapping.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fdf1c3-721f-4f94-97bb-17ca976f9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in new_mapping:\n",
    "#     data[new_mapping[k]].isel(time=1).plot(figsize=(1,1), add_colorbar=False)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835090e-50a7-455e-9dad-18271d3b16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataset\n",
    "# collate function to make cubelets of the same shape! and do padding with zeros!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34efff-ae06-4cb8-9079-1604b8bed4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CubeletsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, xd: xr.Dataset, xs: xr.Dataset, y:xr.Dataset, cubelet_indices, persist=False, lstm_1d = False, static_to_dynamic=False):\n",
    "        \n",
    "        self.xd = xd\n",
    "        self.y = y\n",
    "        self.xs = xs\n",
    "        \n",
    "        self.xd = self.xd.to_stacked_array( new_dim=\"feat\", sample_dims = [\"time\", \"lat\", \"lon\"]) # time, lat, lon , feat\n",
    "        self.xd = self.xd.transpose(\"time\", \"feat\", \"lat\" , \"lon\") # T C H W\n",
    "\n",
    "        self.y = self.y.to_stacked_array( new_dim=\"feat\", sample_dims = [\"time\", \"lat\", \"lon\"])\n",
    "        self.y = self.y.transpose(\"time\", \"feat\", \"lat\" , \"lon\") # T C H W\n",
    "\n",
    "        self.xs = xs.to_stacked_array( new_dim=\"feat\", sample_dims = [\"lat\", \"lon\"]) # H W C\n",
    "        self.xs = self.xs.transpose(\"feat\", \"lat\", \"lon\")\n",
    "        \n",
    "        if persist:\n",
    "            self.xd = self.xd.persist()\n",
    "            self.y = self.y.persist()\n",
    "            self.xs = self.xs.persist()\n",
    "\n",
    "        self.lstm_1d = lstm_1d\n",
    "        self.static_to_dynamic = static_to_dynamic\n",
    "        \n",
    "        self.cubelet_indices = cubelet_indices\n",
    "\n",
    "        # expand static to dynamic \n",
    "        #time_da = xr.DataArray(dynamic.time.values, [('time', dynamic.time.values)])\n",
    "        #static = static.expand_dims({\"time\":time_da})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cubelet_indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        cubelet_idx = list(self.cubelet_indices.keys())[index]\n",
    "        \n",
    "        print(cubelet_idx, self.cubelet_indices[cubelet_idx])\n",
    "\n",
    "        time_slice = self.cubelet_indices[cubelet_idx][\"time\"]\n",
    "        lat_slice =  self.cubelet_indices[cubelet_idx][\"lat\"]\n",
    "        lon_slice = self.cubelet_indices[cubelet_idx][\"lon\"]\n",
    "\n",
    "        # L C H W\n",
    "        xd = self.xd[time_slice,:, lat_slice, lon_slice].values\n",
    "        y = self.y[time_slice,:, lat_slice, lon_slice].values\n",
    "        xs = self.xs[:, lat_slice,lon_slice].values\n",
    "        \n",
    "        xd = torch.tensor(xd)\n",
    "        y = torch.tensor(y)\n",
    "        xs = torch.tensor(xs)\n",
    "            \n",
    "        if self.lstm_1d:\n",
    "            xd = xd.flatten(2,3) # L C H W => L C N\n",
    "            xd = torch.permute(xd, (2, 0, 1))\n",
    "            \n",
    "            y = y.flatten(2,3) # L C H W => L C N\n",
    "            y = torch.permute(y, (2, 0, 1))\n",
    "    \n",
    "            xs = xs.flatten(xs, 1,2)\n",
    "                \n",
    "        if self.xs is not None:\n",
    "\n",
    "            if self.static_to_dynamic:\n",
    "                xs = xs.unsqueeze(0).repeat(xs.size(0), 1, 1, 1)\n",
    "            return xd, xs, y\n",
    "        else:\n",
    "            return xd, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c4294-75df-4cd0-83ff-e3506e1cff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(new_sub_mapping.keys())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c321c0-3567-4c97-a0b2-5435e200ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = target.chunk(\"auto\")\n",
    "xs = static.chunk(\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1fb3f4-572c-43fa-ac0f-cbbbe50210d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4da21e-d827-409f-ad2b-31bc9bf8177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CubeletsDataset(data, xs, y, new_sub_mapping, persist=True, lstm_1d=False, static_to_dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc0ec89-7fb1-437d-9976-3a8f28f22ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bdbd6d-b5fb-448f-837e-d2b3becf0ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler \n",
    "from hython.sampler import SubsetRandomSampler, SubsetSequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c5008-a036-4a17-910f-bf9d6bef7224",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SubsetRandomSampler(range(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622e87a7-1ee0-4956-9c3c-4b9ab4988b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampler = SubsetSequentialSampler(range(len(new_mapping.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d3e4e-b2a5-487a-ba0b-3ecb0ef5e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset = dataset , batch_size = 1, sampler=sampler) # for lstm_1d  the batches are decided by lat*lon so here put batch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8efbb06-f718-4034-ae3a-ceca364e5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721476a-7b0f-4240-b6ff-185dfaf64e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(iter(dataloader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa7cab-fd1e-45c3-aba5-b4ddd77101ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tx,ts,ty in dataloader:\n",
    "    print(tx.shape,ts.shape, ty.shape)\n",
    "    fig, axs = plt.subplots(1,3, figsize=(5,5))\n",
    "    axs[0].imshow(tx[0 , 0, 0 , ...])\n",
    "    axs[1].imshow(ts[0 , 0, 0 , ...])\n",
    "    axs[2].imshow(ty[0 , 0, 0 , ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287a668-0b2f-4af2-b3b2-b11ecc459d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a880d3-673a-45c3-9efa-e3ecebe4b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change shape for old lstm\n",
    "\n",
    "res = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5555454-414a-481b-83c9-cbcfae11ddff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aac2e7d-5751-4b66-bf98-f946abcd1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "time, lat ,lon = 360, 32,32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75739384-149c-40ba-aa3c-900f20af8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(\"XBatchDataset\")(\n",
    "                      dynamic_train, \n",
    "                      target_train, \n",
    "                      static_train, \n",
    "                      lstm=False, \n",
    "                      xbatcher_kwargs={ \"input_dims\": {\"time\": time, \"lat\":lat, \"lon\":lon},\n",
    "                                       \"batch_dims\": {\"lat\":lat, \"lon\":lon}, \n",
    "                                       #\"input_overlap\":{\"time\":1},\n",
    "                                       \"concat_input_dims\":False,\n",
    "                                       \"preload_batch\":True})\n",
    "test_dataset = get_dataset(\"XBatchDataset\")(\n",
    "                      dynamic_test, \n",
    "                      target_test, \n",
    "                      static_test, \n",
    "                      lstm=False, \n",
    "                      xbatcher_kwargs={ \"input_dims\": {\"time\": time, \"lat\":lat, \"lon\":lon},\n",
    "                                       \"batch_dims\": {\"lat\":lat, \"lon\":lon}, \n",
    "                                       #\"input_overlap\":{\"time\":1},\n",
    "                                       \"concat_input_dims\":False,\n",
    "                                       \"preload_batch\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df522c2d-f593-47fd-b65e-69fa23c33001",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0e02f-285e-495a-9a86-621a3dcdd2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d94b94-a598-4b04-8422-9967d488067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))[0].shape, next(iter(train_dataloader))[1].shape, next(iter(train_dataloader))[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf95da-2356-47cd-9078-2bbc8aea1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hython.models.convLSTM import ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be28ab-52c0-4ec4-ac77-c44b45c8af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a069b10-3bcd-431e-b926-57d132ba66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvLSTM(\n",
    "    input_dim =  DYNAMIC_INPUT_SIZE + STATIC_INPUT_SIZE,\n",
    "    output_dim= OUTPUT_SIZE,\n",
    "    hidden_dim = (HIDDEN_SIZE),\n",
    "    kernel_size = KERNEL_SIZE,\n",
    "    num_layers = NUM_LSTM_LAYER,\n",
    "    batch_first = True,\n",
    "    bias = True,\n",
    "    return_all_layers = False\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20a06ba-95ef-4a3e-a744-3423a49179ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=10)\n",
    "\n",
    "loss_fn = RMSELoss(target_weight={\"vwc\":0.5, \"actevap\":0.5})\n",
    "metric_fn = MSEMetric(target_names=[\"vwc\", \"actevap\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51771b9-cced-4662-a9b4-70263b9b591b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = XBatcherTrainer(\n",
    "    RNNTrainParams(\n",
    "               experiment=experiment, \n",
    "               temporal_subsampling=False, \n",
    "               temporal_subset=1, \n",
    "               seq_length=SEQ_LENGTH, \n",
    "               target_names=target_names,\n",
    "               metric_func=metric_fn,\n",
    "               loss_func=loss_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde287bf-8c64-4433-809c-20fac92c9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_surr_output = f\"{dir_surr_output}/{experiment}_{surr_model_output}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167093f2-d61a-415a-96b7-6cf37c533f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model, loss_history, metric_history = train_val(\n",
    "    trainer,\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    10,\n",
    "    opt,\n",
    "    lr_scheduler,\n",
    "    file_surr_output,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a9652-e1a3-4f1c-a57e-3aaeae093dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/mnt/CEPH_PROJECTS/InterTwin/hydrologic_data/surrogate_model/exp1_convlstm.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071788eb-05e4-498f-ac4e-284934d3f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993177be-554f-4a47-bddb-e900f4fd0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lepochs = list(range(1, EPOCHS + 1))\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize= (12,6), sharex=True)\n",
    "\n",
    "axs[0].plot(lepochs, metric_history['train_vwc'], marker='.', linestyle='-', color='b', label='Training')\n",
    "axs[0].plot(lepochs, metric_history['val_vwc'], marker='.', linestyle='-', color='r', label='Validation')\n",
    "axs[0].set_title('SM')\n",
    "axs[0].set_ylabel(metric_fn.__class__.__name__)\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "axs[1].plot(lepochs, metric_history['train_actevap'], marker='.', linestyle='-', color='b', label='Training')\n",
    "axs[1].plot(lepochs, metric_history['val_actevap'], marker='.', linestyle='-', color='r', label='Validation')\n",
    "axs[1].set_title('ET')\n",
    "axs[1].set_ylabel(metric_fn.__class__.__name__)\n",
    "axs[1].grid(True)\n",
    "\n",
    "axs[2].plot(lepochs, [i.detach().cpu().numpy() for i in loss_history['train']], marker='.', linestyle='-', color='b', label='Training')\n",
    "axs[2].plot(lepochs, [i.detach().cpu().numpy() for i in loss_history['val']], marker='.', linestyle='-', color='r', label='Validation')\n",
    "axs[2].set_title('Loss')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel(loss_fn.__name__)\n",
    "axs[2].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943e9d8-587e-4d0f-9179-7ab72f9f5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Xd, Xs, model, batch_size, device):\n",
    "    model = model.to(device)\n",
    "    X = torch.concat([Xd, Xs], 2).to(device)\n",
    "    arr = []\n",
    "    for i in range(0, Xd.shape[0], batch_size):\n",
    "        out = model(X)[0]\n",
    "        #import pdb;pdb.set_trace()\n",
    "        arr.append(out[i : (i + batch_size)].detach().cpu().numpy())\n",
    "    return np.vstack(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c025b28-16fb-4328-b057-62f398898c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict(res[0], res[1], model, batch_size=8,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f031f74-3bdb-4b04-b8b4-6a95fefeb776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe92b9a-dd27-4aa4-83f6-4f98ebf42592",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5087d1f-c3ca-48bd-9d6b-e31b93e4e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output[10,-1,:,:,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a445e09-4a85-4116-8bb6-eb0d17103559",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(res[2][0,-1,0,:,:])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2b3d3-24fa-4771-bc46-a3a1d777e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((output[0,-1,:,:,0] - np.array(res[2][0,-1,0,:,:])), cmap=\"RdBu\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993f8e8-34cc-48f2-8921-9aac62bdafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7b83b-da1a-49b0-b8b3-5765471174d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882840e0-1f47-41c3-a76c-5b157fdec87f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emulator",
   "language": "python",
   "name": "emulator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
