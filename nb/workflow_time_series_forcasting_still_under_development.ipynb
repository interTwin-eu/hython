{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30de281b-5aee-4e21-a402-d9b7edbd65ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Eurac utils\n",
    "from hython.hython.datasets.dataset_reader import xarray_to_array, create_dataset\n",
    "from hython.hython.models.lstm import CustomLSTMModel\n",
    "from hython.hython.train_val import train_val\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b688961a-b248-4150-a5c1-00b0ae52dfab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/CEPH_PROJECTS/InterTwin/Surrogate_Model/Train Model/Deltras Data']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob \n",
    "glob.glob('/mnt/CEPH_PROJECTS/InterTwin/Surrogate_Model/Train Model/Deltras Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e32fbd-954e-4d4e-8764-81ce5b6e372f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dyn_vars_ds = xr.open_dataset('/mnt/CEPH_PROJECTS/InterTwin/Surrogate_Model/Train Model/Deltras Data/Dynamic_Data_2000_2015.nc',decode_coords='all')#.to_dataset(dim='variable')\n",
    "static_params_ds = xr.open_dataset('/mnt/CEPH_PROJECTS/InterTwin/Surrogate_Model/Train Model/Deltras Data/staticmaps.nc',decode_coords='all')#.to_dataset(dim='variable')\n",
    "target_ds = xr.open_dataset('/mnt/CEPH_PROJECTS/InterTwin/Surrogate_Model/Train Model/Deltras Data/Target_train_et_sm_2000_2015.nc',decode_coords='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a24a19-3872-4f76-82cf-fbc1b52ce744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stat_parmas = ['dem_subgrid','KsatVer_100.0cm', 'wflow_dem','hydrodem_avg_D8', 'N',\n",
    "               'RootingDepth', 'thetaS','thetaR','KsatVer', 'M_original_', 'f_', 'M_original', 'f']\n",
    "\n",
    "dyn_vars_ds1 = dyn_vars_ds.sel(time=slice(dyn_vars_ds.time[0], dyn_vars_ds.time[1459]))\n",
    "target_ds1 = target_ds.sel(time=slice(target_ds.time[0], target_ds.time[1459]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71be01c7-a6aa-4191-aff9-389e0c86596e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:      (time: 1460, lat: 214, lon: 245)\n",
       "Coordinates:\n",
       "  * time         (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2003-12-30\n",
       "  * lat          (lat) float64 47.09 47.08 47.07 47.06 ... 45.33 45.32 45.31\n",
       "  * lon          (lon) float64 10.29 10.3 10.3 10.31 ... 12.3 12.3 12.31 12.32\n",
       "Data variables:\n",
       "    precip       (time, lat, lon) float32 ...\n",
       "    pet          (time, lat, lon) float32 ...\n",
       "    temp         (time, lat, lon) float32 ...\n",
       "    spatial_ref  (time) int32 ...\n",
       "Attributes:\n",
       "    units:           mm d**-1\n",
       "    long_name:       Total precipitation\n",
       "    category:        meteo\n",
       "    notes:           Extracted from Copernicus Climate Data Store; resampled ...\n",
       "    paper_doi:       10.1002/qj.3803\n",
       "    paper_ref:       Hersbach et al. (2019)\n",
       "    source_license:  https://cds.climate.copernicus.eu/cdsapp/#!/terms/licenc...\n",
       "    source_url:      https://doi.org/10.24381/cds.bd0915c6\n",
       "    source_version:  ERA5 daily data on pressure levels\n",
       "    _FillValue:      nan\n",
       "    unit:            mm\n",
       "    precip_fn:       era5</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-5ae2658e-cec7-4183-aa13-60f97a7246f5' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-5ae2658e-cec7-4183-aa13-60f97a7246f5' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 1460</li><li><span class='xr-has-index'>lat</span>: 214</li><li><span class='xr-has-index'>lon</span>: 245</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-9b9ad621-a6a9-433e-8e2b-9c718f564157' class='xr-section-summary-in' type='checkbox'  checked><label for='section-9b9ad621-a6a9-433e-8e2b-9c718f564157' class='xr-section-summary' >Coordinates: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2000-01-01 ... 2003-12-30</div><input id='attrs-ab558f03-36f8-4924-aad3-d911b6bae18f' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-ab558f03-36f8-4924-aad3-d911b6bae18f' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-2493652a-4d06-449d-8078-9118ad2e7fcc' class='xr-var-data-in' type='checkbox'><label for='data-2493652a-4d06-449d-8078-9118ad2e7fcc' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;2000-01-01T00:00:00.000000000&#x27;, &#x27;2000-01-02T00:00:00.000000000&#x27;,\n",
       "       &#x27;2000-01-03T00:00:00.000000000&#x27;, ..., &#x27;2003-12-28T00:00:00.000000000&#x27;,\n",
       "       &#x27;2003-12-29T00:00:00.000000000&#x27;, &#x27;2003-12-30T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lat</span></div><div class='xr-var-dims'>(lat)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>47.09 47.08 47.07 ... 45.32 45.31</div><input id='attrs-8d468136-ee4b-48d6-aa6d-fab87c3dc884' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-8d468136-ee4b-48d6-aa6d-fab87c3dc884' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b8801223-f1cc-4829-b5b9-0f322871ed56' class='xr-var-data-in' type='checkbox'><label for='data-b8801223-f1cc-4829-b5b9-0f322871ed56' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>long_name :</span></dt><dd>latitude coordinate</dd><dt><span>short_name :</span></dt><dd>lat</dd><dt><span>units :</span></dt><dd>degrees_north</dd></dl></div><div class='xr-var-data'><pre>array([47.0875  , 47.079167, 47.070833, ..., 45.329167, 45.320833, 45.3125  ])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lon</span></div><div class='xr-var-dims'>(lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>10.29 10.3 10.3 ... 12.31 12.32</div><input id='attrs-d064b6d2-e114-45b3-925e-4c2bf7926a84' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-d064b6d2-e114-45b3-925e-4c2bf7926a84' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-c0055197-2989-46d2-9922-286f243fe532' class='xr-var-data-in' type='checkbox'><label for='data-c0055197-2989-46d2-9922-286f243fe532' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>long_name :</span></dt><dd>longitude coordinate</dd><dt><span>short_name :</span></dt><dd>lon</dd><dt><span>units :</span></dt><dd>degrees_east</dd></dl></div><div class='xr-var-data'><pre>array([10.2875  , 10.295833, 10.304167, ..., 12.304167, 12.3125  , 12.320833])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-47f2eb1f-2bc6-4acb-96b1-6d1a6c6f4675' class='xr-section-summary-in' type='checkbox'  checked><label for='section-47f2eb1f-2bc6-4acb-96b1-6d1a6c6f4675' class='xr-section-summary' >Data variables: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>precip</span></div><div class='xr-var-dims'>(time, lat, lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-c0619637-48d6-4f32-b6cb-0a701a06013e' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-c0619637-48d6-4f32-b6cb-0a701a06013e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f1ead68a-de6c-4b20-a1e7-4afa57a4b9ef' class='xr-var-data-in' type='checkbox'><label for='data-f1ead68a-de6c-4b20-a1e7-4afa57a4b9ef' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>mm d**-1</dd><dt><span>long_name :</span></dt><dd>Total precipitation</dd><dt><span>category :</span></dt><dd>meteo</dd><dt><span>notes :</span></dt><dd>Extracted from Copernicus Climate Data Store; resampled by Deltares to daily frequency</dd><dt><span>paper_doi :</span></dt><dd>10.1002/qj.3803</dd><dt><span>paper_ref :</span></dt><dd>Hersbach et al. (2019)</dd><dt><span>source_license :</span></dt><dd>https://cds.climate.copernicus.eu/cdsapp/#!/terms/licence-to-use-copernicus-products</dd><dt><span>source_url :</span></dt><dd>https://doi.org/10.24381/cds.bd0915c6</dd><dt><span>source_version :</span></dt><dd>ERA5 daily data on pressure levels</dd><dt><span>unit :</span></dt><dd>mm</dd><dt><span>precip_fn :</span></dt><dd>era5</dd></dl></div><div class='xr-var-data'><pre>[76547800 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>pet</span></div><div class='xr-var-dims'>(time, lat, lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-9daac849-4f30-4d56-8757-1f8e47ed1916' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-9daac849-4f30-4d56-8757-1f8e47ed1916' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f345d9dc-5733-42ff-8139-f2c46c488eb0' class='xr-var-data-in' type='checkbox'><label for='data-f345d9dc-5733-42ff-8139-f2c46c488eb0' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>unit :</span></dt><dd>mm</dd><dt><span>pet_fn :</span></dt><dd>era5</dd><dt><span>pet_method :</span></dt><dd>debruin</dd></dl></div><div class='xr-var-data'><pre>[76547800 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>temp</span></div><div class='xr-var-dims'>(time, lat, lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-2401130d-85e6-4080-9bfc-5ee80dfb919e' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-2401130d-85e6-4080-9bfc-5ee80dfb919e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-a4901138-6ba6-44f1-992f-b656bfa82547' class='xr-var-data-in' type='checkbox'><label for='data-a4901138-6ba6-44f1-992f-b656bfa82547' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>unit :</span></dt><dd>degree C.</dd><dt><span>temp_fn :</span></dt><dd>era5</dd><dt><span>temp_correction :</span></dt><dd>True</dd></dl></div><div class='xr-var-data'><pre>[76547800 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>spatial_ref</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>int32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-15ca7bb5-b7c0-40cf-80db-645928ebee7f' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-15ca7bb5-b7c0-40cf-80db-645928ebee7f' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f18dbb30-9e66-498e-a4ce-15696109a6b2' class='xr-var-data-in' type='checkbox'><label for='data-f18dbb30-9e66-498e-a4ce-15696109a6b2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>crs_wkt :</span></dt><dd>GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563]],PRIMEM[&quot;Greenwich&quot;,0],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AXIS[&quot;Latitude&quot;,NORTH],AXIS[&quot;Longitude&quot;,EAST],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]]</dd><dt><span>semi_major_axis :</span></dt><dd>6378137.0</dd><dt><span>semi_minor_axis :</span></dt><dd>6356752.314245179</dd><dt><span>inverse_flattening :</span></dt><dd>298.257223563</dd><dt><span>reference_ellipsoid_name :</span></dt><dd>WGS 84</dd><dt><span>longitude_of_prime_meridian :</span></dt><dd>0.0</dd><dt><span>prime_meridian_name :</span></dt><dd>Greenwich</dd><dt><span>geographic_crs_name :</span></dt><dd>WGS 84</dd><dt><span>grid_mapping_name :</span></dt><dd>latitude_longitude</dd><dt><span>spatial_ref :</span></dt><dd>GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563]],PRIMEM[&quot;Greenwich&quot;,0],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AXIS[&quot;Latitude&quot;,NORTH],AXIS[&quot;Longitude&quot;,EAST],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]]</dd><dt><span>x_dim :</span></dt><dd>lon</dd><dt><span>y_dim :</span></dt><dd>lat</dd><dt><span>GeoTransform :</span></dt><dd>10.283333333333339 0.008333333333333316 0.0 47.09166666666667 0.0 -0.008333333333333358</dd><dt><span>dim0 :</span></dt><dd>time</dd></dl></div><div class='xr-var-data'><pre>[1460 values with dtype=int32]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-5818cd76-35f4-411d-ac47-9c0deab574a4' class='xr-section-summary-in' type='checkbox'  ><label for='section-5818cd76-35f4-411d-ac47-9c0deab574a4' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-b45e62a3-0044-42a2-9723-7d632ea9494e' class='xr-index-data-in' type='checkbox'/><label for='index-b45e62a3-0044-42a2-9723-7d632ea9494e' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2000-01-01&#x27;, &#x27;2000-01-02&#x27;, &#x27;2000-01-03&#x27;, &#x27;2000-01-04&#x27;,\n",
       "               &#x27;2000-01-05&#x27;, &#x27;2000-01-06&#x27;, &#x27;2000-01-07&#x27;, &#x27;2000-01-08&#x27;,\n",
       "               &#x27;2000-01-09&#x27;, &#x27;2000-01-10&#x27;,\n",
       "               ...\n",
       "               &#x27;2003-12-21&#x27;, &#x27;2003-12-22&#x27;, &#x27;2003-12-23&#x27;, &#x27;2003-12-24&#x27;,\n",
       "               &#x27;2003-12-25&#x27;, &#x27;2003-12-26&#x27;, &#x27;2003-12-27&#x27;, &#x27;2003-12-28&#x27;,\n",
       "               &#x27;2003-12-29&#x27;, &#x27;2003-12-30&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, length=1460, freq=None))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>lat</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-2db94457-2166-4963-ab54-073d0e24a61f' class='xr-index-data-in' type='checkbox'/><label for='index-2db94457-2166-4963-ab54-073d0e24a61f' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Float64Index([           47.0875, 47.079166666666666,  47.07083333333333,\n",
       "                         47.0625,  47.05416666666667, 47.045833333333334,\n",
       "                         47.0375,  47.02916666666667, 47.020833333333336,\n",
       "                         47.0125,\n",
       "              ...\n",
       "              45.387499999999996,  45.37916666666666,  45.37083333333333,\n",
       "                         45.3625, 45.354166666666664,  45.34583333333333,\n",
       "                         45.3375, 45.329166666666666,  45.32083333333333,\n",
       "                         45.3125],\n",
       "             dtype=&#x27;float64&#x27;, name=&#x27;lat&#x27;, length=214))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>lon</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-dbefbd3c-8eac-4935-9144-76e2cd0841ff' class='xr-index-data-in' type='checkbox'/><label for='index-dbefbd3c-8eac-4935-9144-76e2cd0841ff' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Float64Index([10.287500000000005,  10.29583333333334, 10.304166666666672,\n",
       "              10.312500000000005, 10.320833333333338, 10.329166666666673,\n",
       "              10.337500000000006, 10.345833333333339, 10.354166666666671,\n",
       "              10.362500000000006,\n",
       "              ...\n",
       "              12.245833333333335, 12.254166666666668, 12.262500000000003,\n",
       "              12.270833333333336, 12.279166666666669, 12.287500000000001,\n",
       "              12.295833333333334, 12.304166666666669, 12.312500000000002,\n",
       "              12.320833333333335],\n",
       "             dtype=&#x27;float64&#x27;, name=&#x27;lon&#x27;, length=245))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-718a3019-11e9-4e2c-8bf6-16e295f7d3f7' class='xr-section-summary-in' type='checkbox'  ><label for='section-718a3019-11e9-4e2c-8bf6-16e295f7d3f7' class='xr-section-summary' >Attributes: <span>(12)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>mm d**-1</dd><dt><span>long_name :</span></dt><dd>Total precipitation</dd><dt><span>category :</span></dt><dd>meteo</dd><dt><span>notes :</span></dt><dd>Extracted from Copernicus Climate Data Store; resampled by Deltares to daily frequency</dd><dt><span>paper_doi :</span></dt><dd>10.1002/qj.3803</dd><dt><span>paper_ref :</span></dt><dd>Hersbach et al. (2019)</dd><dt><span>source_license :</span></dt><dd>https://cds.climate.copernicus.eu/cdsapp/#!/terms/licence-to-use-copernicus-products</dd><dt><span>source_url :</span></dt><dd>https://doi.org/10.24381/cds.bd0915c6</dd><dt><span>source_version :</span></dt><dd>ERA5 daily data on pressure levels</dd><dt><span>_FillValue :</span></dt><dd>nan</dd><dt><span>unit :</span></dt><dd>mm</dd><dt><span>precip_fn :</span></dt><dd>era5</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:      (time: 1460, lat: 214, lon: 245)\n",
       "Coordinates:\n",
       "  * time         (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2003-12-30\n",
       "  * lat          (lat) float64 47.09 47.08 47.07 47.06 ... 45.33 45.32 45.31\n",
       "  * lon          (lon) float64 10.29 10.3 10.3 10.31 ... 12.3 12.3 12.31 12.32\n",
       "Data variables:\n",
       "    precip       (time, lat, lon) float32 ...\n",
       "    pet          (time, lat, lon) float32 ...\n",
       "    temp         (time, lat, lon) float32 ...\n",
       "    spatial_ref  (time) int32 ...\n",
       "Attributes:\n",
       "    units:           mm d**-1\n",
       "    long_name:       Total precipitation\n",
       "    category:        meteo\n",
       "    notes:           Extracted from Copernicus Climate Data Store; resampled ...\n",
       "    paper_doi:       10.1002/qj.3803\n",
       "    paper_ref:       Hersbach et al. (2019)\n",
       "    source_license:  https://cds.climate.copernicus.eu/cdsapp/#!/terms/licenc...\n",
       "    source_url:      https://doi.org/10.24381/cds.bd0915c6\n",
       "    source_version:  ERA5 daily data on pressure levels\n",
       "    _FillValue:      nan\n",
       "    unit:            mm\n",
       "    precip_fn:       era5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyn_vars_ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b06ca4d0-e071-4a08-af09-209213b000c2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52430, 1460, 2)\n",
      "Done target parmas\n",
      "(52430, 1460, 3)\n",
      "Done dynamics parmas\n",
      "(52430, 13)\n",
      "Done static parmas\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# # Create the dataset and return DataLoader\n",
    "target_arr, static_params_arr, dyn_vars_arr   = create_dataset(dyn_vars_ds1, static_params_ds, target_ds1,\n",
    "                                            batch_size=8,\n",
    "                                            time_steps=1460, #time_steps=365 5844\n",
    "                                            dyn_vars_names=['precip', 'pet', 'temp'],\n",
    "                                            static_params_names=stat_parmas, #[ 'M', 'thetaS', 'RootingDepth', 'Kext', 'Sl', 'Swood', 'TT', 'KsatHorFrac'],\n",
    "                                            target_names=['soil_moisture', 'evapotranspiration']) # ['vwc_percroot', 'soil_moisture', 'evapotranspiration']\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# #/mnt/CEPH_PROJECTS/InterTwin/Surrogate_Model/Train Model/preprocessed_data/target_arr.npy\n",
    "np.save('/mnt/CEPH_PROJECTS/InterTwin/Surrogate_Model/Train Model/preprocessed_data/static_params_arr_4years.npy', static_params_arr)\n",
    "np.save('/mnt/CEPH_PROJECTS/InterTwin/Surrogate_Model/Train Model/preprocessed_data/target_arr_4years.npy', target_arr)\n",
    "np.save('/mnt/CEPH_PROJECTS/InterTwin/Surrogate_Model/Train Model/preprocessed_data/dyn_vars_arr_4years.npy', dyn_vars_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ffb7d78-3799-4401-9d5c-eb8d2005f3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f83aeb94-f425-4999-8312-1f9395502590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x1 = dyn_vars_arr\n",
    "# x2 = static_params_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9236c48-f84d-4874-859f-7d87db660725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20051, 1460, 16])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_params_arr1 = static_params_arr.unsqueeze(1).expand(-1, dyn_vars_arr.shape[1], -1)\n",
    "all_vars_arr = torch.cat((dyn_vars_arr, static_params_arr1), -1)\n",
    "all_vars_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "915d5380-7596-4c5d-8882-e182332ad22d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_seq_length = 365\n",
    "batch_size = 8\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = dyn_vars_arr[:, :-validation_seq_length, :]\n",
    "train_targets = target_arr[:, :-validation_seq_length, :]\n",
    "\n",
    "val_data = dyn_vars_arr[:, -validation_seq_length:, :]\n",
    "val_targets = target_arr[:, -validation_seq_length:, :]\n",
    "\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(train_data, train_targets)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(val_data, val_targets)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2076805c-c935-4fda-9a2a-ed06d68aba8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20051, 1095, 3])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ccd5bbb6-3906-464d-b026-dc0a948f6787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7217c0b2-4a43-45f3-b617-83dc5628f55d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "009fab62-6639-419e-9c3a-10681de60119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        \n",
    "        self.mseloss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Root Mean Squared Error (RMSE) between two tensors.\n",
    "\n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): The true values.\n",
    "        y_pred (torch.Tensor): The predicted values.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The RMSE loss.\n",
    "        \"\"\"\n",
    "        rmse_loss = torch.sqrt(self.mseloss(y_true, y_pred))\n",
    "\n",
    "        return rmse_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ce8bfedf-5d78-4edc-b8fd-de56fe783948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = RMSELoss() #nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "91317ae0-2463-41a0-8361-a87d0165320d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch [1/50]\n",
      "Train Loss: 7.7360\n",
      "Validation RMSE: 29.330810546875\n",
      "######### Epoch [2/50]\n",
      "Train Loss: 7.6811\n",
      "Validation RMSE: 29.457853317260742\n",
      "######### Epoch [3/50]\n",
      "Train Loss: 7.5923\n",
      "Validation RMSE: 27.814800262451172\n",
      "######### Epoch [4/50]\n",
      "Train Loss: 7.5806\n",
      "Validation RMSE: 30.020627975463867\n",
      "######### Epoch [5/50]\n",
      "Train Loss: 7.5392\n",
      "Validation RMSE: 29.01734733581543\n",
      "######### Epoch [6/50]\n",
      "Train Loss: 7.5676\n",
      "Validation RMSE: 26.378807067871094\n",
      "######### Epoch [7/50]\n",
      "Train Loss: 7.4542\n",
      "Validation RMSE: 24.62902069091797\n",
      "######### Epoch [8/50]\n",
      "Train Loss: 7.4299\n",
      "Validation RMSE: 26.0306339263916\n",
      "######### Epoch [9/50]\n",
      "Train Loss: 7.4829\n",
      "Validation RMSE: 26.76091957092285\n",
      "######### Epoch [10/50]\n",
      "Train Loss: 7.4695\n",
      "Validation RMSE: 23.515878677368164\n",
      "######### Epoch [11/50]\n",
      "Train Loss: 7.5039\n",
      "Validation RMSE: 24.625141143798828\n",
      "######### Epoch [12/50]\n",
      "Train Loss: 7.4686\n",
      "Validation RMSE: 26.890583038330078\n",
      "######### Epoch [13/50]\n",
      "Train Loss: 7.5032\n",
      "Validation RMSE: 24.290369033813477\n",
      "######### Epoch [14/50]\n",
      "Train Loss: 7.4518\n",
      "Validation RMSE: 23.419496536254883\n",
      "######### Epoch [15/50]\n",
      "Train Loss: 7.6277\n",
      "Validation RMSE: 26.370473861694336\n",
      "######### Epoch [16/50]\n",
      "Train Loss: 7.4991\n",
      "Validation RMSE: 24.20736312866211\n",
      "######### Epoch [17/50]\n",
      "Train Loss: 7.5614\n",
      "Validation RMSE: 24.808679580688477\n",
      "######### Epoch [18/50]\n",
      "Train Loss: 7.5287\n",
      "Validation RMSE: 24.49345588684082\n",
      "######### Epoch [19/50]\n",
      "Train Loss: 7.6521\n",
      "Validation RMSE: 24.64876365661621\n",
      "######### Epoch [20/50]\n",
      "Train Loss: 7.4981\n",
      "Validation RMSE: 21.923158645629883\n",
      "######### Epoch [21/50]\n",
      "Train Loss: 7.5636\n",
      "Validation RMSE: 24.9830265045166\n",
      "######### Epoch [22/50]\n",
      "Train Loss: 7.5577\n",
      "Validation RMSE: 20.301984786987305\n",
      "######### Epoch [23/50]\n",
      "Train Loss: 7.4815\n",
      "Validation RMSE: 25.900941848754883\n",
      "######### Epoch [24/50]\n",
      "Train Loss: 7.4832\n",
      "Validation RMSE: 23.606597900390625\n",
      "######### Epoch [25/50]\n",
      "Train Loss: 7.4433\n",
      "Validation RMSE: 23.227685928344727\n",
      "######### Epoch [26/50]\n",
      "Train Loss: 7.4653\n",
      "Validation RMSE: 22.349348068237305\n",
      "######### Epoch [27/50]\n",
      "Train Loss: 7.6014\n",
      "Validation RMSE: 27.096599578857422\n",
      "######### Epoch [28/50]\n",
      "Train Loss: 7.6246\n",
      "Validation RMSE: 26.607526779174805\n",
      "######### Epoch [29/50]\n",
      "Train Loss: 7.6808\n",
      "Validation RMSE: 22.210262298583984\n",
      "######### Epoch [30/50]\n",
      "Train Loss: 7.7396\n",
      "Validation RMSE: 18.429004669189453\n",
      "######### Epoch [31/50]\n",
      "Train Loss: 7.7085\n",
      "Validation RMSE: 23.016401290893555\n",
      "######### Epoch [32/50]\n",
      "Train Loss: 7.6691\n",
      "Validation RMSE: 22.860942840576172\n",
      "######### Epoch [33/50]\n",
      "Train Loss: 7.5617\n",
      "Validation RMSE: 25.769624710083008\n",
      "######### Epoch [34/50]\n",
      "Train Loss: 7.5898\n",
      "Validation RMSE: 24.057287216186523\n",
      "######### Epoch [35/50]\n",
      "Train Loss: 7.4755\n",
      "Validation RMSE: 22.650375366210938\n",
      "######### Epoch [36/50]\n",
      "Train Loss: 7.7288\n",
      "Validation RMSE: 25.531164169311523\n",
      "######### Epoch [37/50]\n",
      "Train Loss: 7.6004\n",
      "Validation RMSE: 26.852890014648438\n",
      "######### Epoch [38/50]\n",
      "Train Loss: 7.5738\n",
      "Validation RMSE: 26.636964797973633\n",
      "######### Epoch [39/50]\n",
      "Train Loss: 7.6414\n",
      "Validation RMSE: 26.95241355895996\n",
      "######### Epoch [40/50]\n",
      "Train Loss: 7.6148\n",
      "Validation RMSE: 30.295631408691406\n",
      "######### Epoch [41/50]\n",
      "Train Loss: 7.5485\n",
      "Validation RMSE: 29.201702117919922\n",
      "######### Epoch [42/50]\n",
      "Train Loss: 7.5201\n",
      "Validation RMSE: 26.20892906188965\n",
      "######### Epoch [43/50]\n",
      "Train Loss: 7.6063\n",
      "Validation RMSE: 31.256040573120117\n",
      "######### Epoch [44/50]\n",
      "Train Loss: 7.9034\n",
      "Validation RMSE: 29.236095428466797\n",
      "######### Epoch [45/50]\n",
      "Train Loss: 7.6659\n",
      "Validation RMSE: 27.15975570678711\n",
      "######### Epoch [46/50]\n",
      "Train Loss: 7.5824\n",
      "Validation RMSE: 29.37537384033203\n",
      "######### Epoch [47/50]\n",
      "Train Loss: 7.5936\n",
      "Validation RMSE: 28.872072219848633\n",
      "######### Epoch [48/50]\n",
      "Train Loss: 7.5890\n",
      "Validation RMSE: 25.670515060424805\n",
      "######### Epoch [49/50]\n",
      "Train Loss: 7.5884\n",
      "Validation RMSE: 25.303390502929688\n",
      "######### Epoch [50/50]\n",
      "Train Loss: 7.6590\n",
      "Validation RMSE: 27.729597091674805\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############### just the dynamic variables ####################\n",
    "\n",
    "\n",
    "# Training loop with RMSE monitoring\n",
    "train_losses = []  # To store training losses\n",
    "val_losses = []    # To store validation losses\n",
    "val_rmse_values = []  # To store RMSE values for the first target during validation\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'######### Epoch [{epoch+1}/{num_epochs}]')\n",
    "    model.train()\n",
    "    train_loss_sum = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs = inputs.view(-1, seq_length - validation_seq_length, input_size)\n",
    "        targets = targets.view(-1, seq_length - validation_seq_length, output_size)\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, targets.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    train_loss_avg = train_loss_sum / num_batches\n",
    "    train_losses.append(train_loss_avg)\n",
    "\n",
    "    #if (epoch + 1) % 10 == 0:\n",
    "    print(f'Train Loss: {train_loss_avg:.4f}')\n",
    "\n",
    "#     # Validation\n",
    "    model.eval()\n",
    "    vals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in val_dataloader:\n",
    "            inputs = inputs.view(-1, validation_seq_length, input_size)\n",
    "            val_outputs = model(inputs.to(device)).cpu().numpy()\n",
    "            vals.append(val_outputs)\n",
    "            \n",
    "        val_predictions = np.vstack(vals)\n",
    "        \n",
    "        rmse_metric = mean_squared_error(val_targets[:,:,0], val_predictions[:,:,0], squared=False)\n",
    "        val_rmse_values.append(rmse_metric)\n",
    "        print(f'Validation RMSE: {rmse_metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d0979d2-5c8c-4cb1-ae23-2b190870d547",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch [1/50]\n",
      "Train Loss: 15.6415\n",
      "Validation RMSE: 20.020015716552734\n",
      "######### Epoch [2/50]\n",
      "Train Loss: 15.3132\n",
      "Validation RMSE: 19.903535842895508\n",
      "######### Epoch [3/50]\n",
      "Train Loss: 15.3060\n",
      "Validation RMSE: 19.682777404785156\n",
      "######### Epoch [4/50]\n",
      "Train Loss: 15.3088\n",
      "Validation RMSE: 19.937238693237305\n",
      "######### Epoch [5/50]\n",
      "Train Loss: 15.3103\n",
      "Validation RMSE: 20.335386276245117\n",
      "######### Epoch [6/50]\n",
      "Train Loss: 15.3074\n",
      "Validation RMSE: 19.881832122802734\n",
      "######### Epoch [7/50]\n",
      "Train Loss: 15.3056\n",
      "Validation RMSE: 19.88032341003418\n",
      "######### Epoch [8/50]\n",
      "Train Loss: 15.3015\n",
      "Validation RMSE: 20.159563064575195\n",
      "######### Epoch [9/50]\n",
      "Train Loss: 15.3011\n",
      "Validation RMSE: 19.980215072631836\n",
      "######### Epoch [10/50]\n",
      "Train Loss: 15.2840\n",
      "Validation RMSE: 19.718652725219727\n",
      "######### Epoch [11/50]\n",
      "Train Loss: 15.2925\n",
      "Validation RMSE: 20.010536193847656\n",
      "######### Epoch [12/50]\n",
      "Train Loss: 15.2916\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, _ \u001b[38;5;129;01min\u001b[39;00m val_dataloader:\n\u001b[1;32m     35\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, validation_seq_length, input_size)\n\u001b[0;32m---> 36\u001b[0m     val_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     37\u001b[0m     vals\u001b[38;5;241m.\u001b[39mappend(val_outputs)\n\u001b[1;32m     39\u001b[0m val_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(vals)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############### with the static variables ####################\n",
    "\n",
    "# Training loop with RMSE monitoring\n",
    "train_losses = []  # To store training losses\n",
    "val_losses = []    # To store validation losses\n",
    "val_rmse_values = []  # To store RMSE values for the first target during validation\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'######### Epoch [{epoch+1}/{num_epochs}]')\n",
    "    model.train()\n",
    "    train_loss_sum = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs = inputs.view(-1, seq_length - validation_seq_length, input_size)\n",
    "        targets = targets.view(-1, seq_length - validation_seq_length, output_size)\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, targets.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    train_loss_avg = train_loss_sum / num_batches\n",
    "    train_losses.append(train_loss_avg)\n",
    "\n",
    "    #if (epoch + 1) % 10 == 0:\n",
    "    print(f'Train Loss: {train_loss_avg:.4f}')\n",
    "\n",
    "#     # Validation\n",
    "    model.eval()\n",
    "    vals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in val_dataloader:\n",
    "            inputs = inputs.view(-1, validation_seq_length, input_size)\n",
    "            val_outputs = model(inputs.to(device)).cpu().numpy()\n",
    "            vals.append(val_outputs)\n",
    "            \n",
    "        val_predictions = np.vstack(vals)\n",
    "        \n",
    "        rmse_metric = mean_squared_error(val_targets[:,:,0], val_predictions[:,:,0], squared=False)\n",
    "        val_rmse_values.append(rmse_metric)\n",
    "        print(f'Validation RMSE: {rmse_metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92008fbb-a037-4c9e-a8ee-410d962c5832",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 17.3820\n",
      "Epoch [2/50], Loss: 16.8013\n",
      "Epoch [3/50], Loss: 4.9605\n",
      "Epoch [4/50], Loss: 18.3184\n",
      "Epoch [5/50], Loss: 12.2493\n",
      "Epoch [6/50], Loss: 17.9480\n",
      "Epoch [7/50], Loss: 9.5517\n",
      "Epoch [8/50], Loss: 19.2007\n",
      "Epoch [9/50], Loss: 6.9690\n",
      "Epoch [10/50], Loss: 5.7845\n",
      "Epoch [11/50], Loss: 17.7766\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_length \u001b[38;5;241m-\u001b[39m validation_seq_length, input_size)\n\u001b[1;32m      8\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_length \u001b[38;5;241m-\u001b[39m validation_seq_length, output_size)\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_gpus/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     12\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 13\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_gpus/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_gpus/lib/python3.9/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Training loop\n",
    "# num_epochs = 50\n",
    "# seq_length = 1460\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for inputs, targets in train_dataloader:\n",
    "#         inputs = inputs.view(-1, seq_length - validation_seq_length, input_size)\n",
    "#         targets = targets.view(-1, seq_length - validation_seq_length, output_size)\n",
    "#         outputs = model(inputs.to(device))\n",
    "#         loss = criterion(outputs, targets.to(device))\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     #if (epoch + 1) % 10 == 0:\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff2782-5a2e-41c9-8f02-74b8809d7309",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2d171784-9cca-42da-bdac-7b3e4197a0f7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52430, 1095, 2)\n",
      "Done target parmas\n",
      "(52430, 1095, 3)\n",
      "Done dynamics parmas\n",
      "(52430, 13)\n",
      "Done static parmas\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset and return DataLoader\n",
    "train_loader, val_loader  = create_dataset(dyn_vars_ds1, static_params_ds, target_ds1,\n",
    "                                           batch_size=8,\n",
    "                                           time_steps=1460, #time_steps=365 5844\n",
    "                                           dyn_vars_names=['precip', 'pet', 'temp'],\n",
    "                                           static_params_names=stat_parmas, #[ 'M', 'thetaS', 'RootingDepth', 'Kext', 'Sl', 'Swood', 'TT', 'KsatHorFrac'],\n",
    "                                           target_names=['soil_moisture', 'evapotranspiration']) # ['vwc_percroot', 'soil_moisture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f7153d9-bfbe-402e-baea-7ca0dda3a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dyn_vars_ds = xr.open_dataset('./data/dyn_vars.nc',decode_coords='all')#.to_dataset(dim='variable')\n",
    "# static_params_ds = xr.open_dataset('./data/staticmaps_calibrated_parameters.nc',decode_coords='all')#.to_dataset(dim='variable')\n",
    "# target_ds = xr.open_dataset('./data/soil_moisture_2019.nc',decode_coords='all')\n",
    "\n",
    "# # Create the dataset and return DataLoader\n",
    "# train_loader, val_loader  = create_dataset(dyn_vars_ds, static_params_ds, target_ds,batch_size=8,\n",
    "#                                            time_steps=365, #time_steps=365 5844\n",
    "#                                            dyn_vars_names=['precip', 'pet', 'temp'],\n",
    "#                                            static_params_names=[ 'M', 'thetaS', 'RootingDepth', 'Kext', 'Sl', 'Swood', 'TT', 'KsatHorFrac'],\n",
    "#                                            target_names=['vwc_percroot']) # ['vwc_percroot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f402a5a5-05ab-4e94-a249-1b4cf5568ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1095, 3])\n",
      "torch.Size([8, 13])\n",
      "torch.Size([8, 1095, 2])\n"
     ]
    }
   ],
   "source": [
    "# Invistigate the dataset \n",
    "for x1, x2, y in train_loader:\n",
    "    print(x1.shape)\n",
    "    print(x2.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "47c20c1f-32fd-442d-8ed4-5f6752717bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Create the model \n",
    "# model_params={\n",
    "#     \"input_size\": 3, #number of dynamic predictors - user_input \n",
    "#     \"number_static_predictors\": 85, #number of static parameters - user_input \n",
    "#     \"hidden_size\": 256, # user_input\n",
    "#     \"output_size\": 2, # number_target - user_input\n",
    "\n",
    "# }\n",
    "\n",
    "# model = CustomLSTMModel(model_params)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "360a8888-2c78-4eb4-b5b1-ee5ed722d030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from hython.hython.models.CudnnLstmModel import CudnnLstmModel\n",
    "# nx = 3 + 2\n",
    "# ny = 1\n",
    "# hiddenSizeLst = 256\n",
    "# WARM_UP_DAY = 10\n",
    "\n",
    "# model = CudnnLstmModel(nx=nx, ny=ny, hiddenSize=hiddenSizeLst)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a9412dc3-c384-4b8c-97d5-bbbc41ee05f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mse_metric(output, target):\n",
    "    metric_epoch = mean_squared_error(output[:,:,0], target[:,:,0], squared=False)\n",
    "    return metric_epoch\n",
    "\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        \n",
    "        self.mseloss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Root Mean Squared Error (RMSE) between two tensors.\n",
    "\n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): The true values.\n",
    "        y_pred (torch.Tensor): The predicted values.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The RMSE loss.\n",
    "        \"\"\"\n",
    "        rmse_loss = torch.sqrt(self.mseloss(y_true, y_pred))\n",
    "\n",
    "        return rmse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8b8fb007-5375-414b-9a58-99af2af68d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Training and validation \n",
    "# path2models= \"./checkpoints\" #./output/kaggle/working/AI4EO/models\n",
    "# if not os.path.exists(path2models):\n",
    "#     os.mkdir(path2models)\n",
    "    \n",
    "    \n",
    "# ## Where to save the trained models weights \n",
    "# ## Set the optimization algorithms and learning rate\n",
    "# opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# ## Set the loss function\n",
    "# #loss_fn = nn.MSELoss()\n",
    "# loss_fn = RMSELoss()\n",
    "# # # 2 targets\n",
    "# # def mse_metric(output, target):\n",
    "# #     metric_epoch = mean_squared_error(output[:,:,0], target[:,:,0])\n",
    "# #     return metric_epoch\n",
    "\n",
    "# # 1 target\n",
    "# def mse_metric(output, target):\n",
    "#     metric_epoch = mean_squared_error(output, target, squared=False)\n",
    "#     return metric_epoch\n",
    "\n",
    "# ## Set the metric function - here using the same loss function \n",
    "# metric_fn = mse_metric #nn.MSELoss()\n",
    "\n",
    "# ## Set the learning rate scheduler\n",
    "# lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=5)\n",
    "\n",
    "# ## Set the training parameters\n",
    "# params_train={\n",
    "#     \"num_epochs\": 150,\n",
    "#     \"optimizer\": opt,\n",
    "#     \"loss_func\": loss_fn,\n",
    "#     \"metric_func\": metric_fn,\n",
    "#     \"train_dl\": train_loader, \n",
    "#     \"val_dl\": val_loader,\n",
    "#     \"sanity_check\": False,\n",
    "#     \"lr_scheduler\": lr_scheduler,\n",
    "#     \"path2weights\": f\"{path2models}/weights.pt\"\n",
    "\n",
    "# }\n",
    "\n",
    "# ## The used device for training\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fea578f7-bace-4bc7-b8d3-439be5dc6547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # one target\n",
    "# model, loss_history = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f2e5d960-6560-4c9b-85fb-7d37093d5d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CustomLSTMModel(nn.Module):\n",
    "#     def __init__(self, model_params):\n",
    "        \n",
    "#         dyn_size  = model_params[\"dyn_size\"]\n",
    "#         hidden_size = model_params[\"hidden_size\"]\n",
    "#         output_size = model_params[\"output_size\"]\n",
    "#         number_static_predictors = model_params[\"number_static_predictors\"]\n",
    "        \n",
    "#         super(CustomLSTMModel, self).__init__()\n",
    "#         self.fc1 = nn.Linear(dyn_size + number_static_predictors, 256)\n",
    "        \n",
    "#         self.lstm = nn.LSTM(256, hidden_size, batch_first=True)\n",
    "        \n",
    "#           # Concatenating with static parameters\n",
    "        \n",
    "#         self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x, static_params):\n",
    "#         x1 = static_params.unsqueeze(1).expand(-1, x.shape[1], -1)\n",
    "#         #print(x.shape)\n",
    "#         #print(x1.shape)\n",
    "#         x = torch.cat((x, x1), -1)\n",
    "        \n",
    "#         out = self.fc1(x)\n",
    "        \n",
    "#         lstm_output, _ = self.lstm(out)\n",
    "        \n",
    "#         # Concatenate LSTM output with static parameters\n",
    "#         #combined_output = torch.cat((lstm_output, static_params.unsqueeze(1).repeat(1, lstm_output.size(1), 1)), dim=-1)\n",
    "        \n",
    "#         out = torch.relu(self.fc2(lstm_output))\n",
    "        \n",
    "        \n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1c9e8161-afe0-44e6-b3de-9e71ec669804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Training and validation \n",
    "# path2models= \"./eurac_checkpoints\" #./output/kaggle/working/AI4EO/models\n",
    "# if not os.path.exists(path2models):\n",
    "#     os.mkdir(path2models)\n",
    "    \n",
    "    \n",
    "# ## Where to save the trained models weights \n",
    "# ## Set the optimization algorithms and learning rate\n",
    "# opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# ## Set the loss function\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "# ## Set the metric function - here using the same loss function \n",
    "# metric_fn = mse_metric #nn.MSELoss()\n",
    "\n",
    "# ## Set the learning rate scheduler\n",
    "# lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=5)\n",
    "\n",
    "# ## Set the training parameters\n",
    "# params_train={\n",
    "#     \"num_epochs\": 50,\n",
    "#     \"optimizer\": opt,\n",
    "#     \"loss_func\": loss_fn,\n",
    "#     \"metric_func\": metric_fn,\n",
    "#     \"train_dl\": train_loader, \n",
    "#     \"val_dl\": val_loader,\n",
    "#     \"sanity_check\": False,\n",
    "#     \"lr_scheduler\": lr_scheduler,\n",
    "#     \"path2weights\": f\"{path2models}/weights.pt\"\n",
    "\n",
    "# }\n",
    "\n",
    "# ## The used device for training\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_params={\n",
    "#     \"dyn_size\": 3, #number of dynamic predictors - user_input \n",
    "#     \"number_static_predictors\": 85, #number of static parameters - user_input \n",
    "#     \"hidden_size\": 256, # user_input\n",
    "#     \"output_size\": 2, # number_target - user_input\n",
    "\n",
    "# }\n",
    "\n",
    "# model = CustomLSTMModel(model_params)\n",
    "# model = model.to(device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "139a7a01-efec-4324-9496-225574a3546b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomLSTMModel(\n",
       "  (lstm): LSTM(3, 256, batch_first=True)\n",
       "  (fc1): Linear(in_features=269, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params={\n",
    "    \"input_size\": 3, #number of dynamic predictors - user_input\n",
    "    \"hidden_size\": 256, # user_input\n",
    "    \"output_size\": 2, # number_target - user_input\n",
    "    \"number_static_predictors\": 13, #number of static parameters - user_input \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "## The used device for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CustomLSTMModel(model_params)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6426a177-da34-4f5f-bb58-80eb2fca1b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training and validation \n",
    "path2models= \"./eurac_checkpoints\" #./output/kaggle/working/AI4EO/models\n",
    "if not os.path.exists(path2models):\n",
    "    os.mkdir(path2models)\n",
    "    \n",
    "    \n",
    "## Where to save the trained models weights \n",
    "## Set the optimization algorithms and learning rate\n",
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "## Set the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "## Set the metric function - here using the same loss function \n",
    "metric_fn = mse_metric #nn.MSELoss()\n",
    "\n",
    "## Set the learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=5)\n",
    "\n",
    "## Set the training parameters\n",
    "params_train={\n",
    "    \"num_epochs\": 50,\n",
    "    \"optimizer\": opt,\n",
    "    \"loss_func\": loss_fn,\n",
    "    \"metric_func\": metric_fn,\n",
    "    \"train_dl\": train_loader, \n",
    "    \"val_dl\": val_loader,\n",
    "    \"sanity_check\": False,\n",
    "    \"lr_scheduler\": lr_scheduler,\n",
    "    \"path2weights\": f\"{path2models}/weights.pt\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "90dbdc1c-9d54-4867-b902-199860a1ccea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 113.67676684505625, train metric: 14.507192611694336\n",
      "val loss: 78.29340853937188, val metric: 12.184865951538086\n",
      "----------\n",
      "Epoch 1/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 80.32493791699112, train metric: 12.35503101348877\n",
      "val loss: 74.94792163642497, val metric: 11.861297607421875\n",
      "----------\n",
      "Epoch 2/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 78.117589836406, train metric: 12.142365455627441\n",
      "val loss: 71.03361775101762, val metric: 11.542475700378418\n",
      "----------\n",
      "Epoch 3/49, current lr=0.01\n",
      "train loss: 76.84526354487697, train metric: 12.043232917785645\n",
      "val loss: 93.298211791277, val metric: 13.408138275146484\n",
      "----------\n",
      "Epoch 4/49, current lr=0.01\n",
      "train loss: 75.46763483699124, train metric: 11.954251289367676\n",
      "val loss: 106.93161516039784, val metric: 14.428851127624512\n",
      "----------\n",
      "Epoch 5/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 70.84691213289103, train metric: 11.564813613891602\n",
      "val loss: 65.69622734638557, val metric: 11.09863567352295\n",
      "----------\n",
      "Epoch 6/49, current lr=0.01\n",
      "train loss: 71.10775367280195, train metric: 11.570961952209473\n",
      "val loss: 76.20591969229699, val metric: 11.981249809265137\n",
      "----------\n",
      "Epoch 7/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 72.07023939575043, train metric: 11.666357040405273\n",
      "val loss: 64.83011665120739, val metric: 11.025979042053223\n",
      "----------\n",
      "Epoch 8/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 68.64818377506703, train metric: 11.39236068725586\n",
      "val loss: 62.99497201644224, val metric: 10.829700469970703\n",
      "----------\n",
      "Epoch 9/49, current lr=0.01\n",
      "train loss: 70.26250990013827, train metric: 11.50818157196045\n",
      "val loss: 64.88127620652321, val metric: 11.027071952819824\n",
      "----------\n",
      "Epoch 10/49, current lr=0.01\n",
      "train loss: 71.20800766314652, train metric: 11.575915336608887\n",
      "val loss: 80.00575982362247, val metric: 12.248346328735352\n",
      "----------\n",
      "Epoch 11/49, current lr=0.01\n",
      "train loss: 73.7610331323676, train metric: 11.791462898254395\n",
      "val loss: 69.02137414346875, val metric: 11.37885856628418\n",
      "----------\n",
      "Epoch 12/49, current lr=0.01\n",
      "train loss: 72.15554450693868, train metric: 11.659549713134766\n",
      "val loss: 81.10855891157107, val metric: 12.375997543334961\n",
      "----------\n",
      "Epoch 13/49, current lr=0.01\n",
      "train loss: 73.21820531735693, train metric: 11.752214431762695\n",
      "val loss: 72.22769210166807, val metric: 11.632467269897461\n",
      "----------\n",
      "Epoch 14/49, current lr=0.01\n",
      "Loading best model weights!\n",
      "train loss: 72.24790738931023, train metric: 11.688587188720703\n",
      "val loss: 65.6865237846365, val metric: 11.115911483764648\n",
      "----------\n",
      "Epoch 15/49, current lr=0.005\n",
      "train loss: 62.74233937679681, train metric: 10.843098640441895\n",
      "val loss: 67.79809129092497, val metric: 11.312081336975098\n",
      "----------\n",
      "Epoch 16/49, current lr=0.005\n",
      "Copied best model weights!\n",
      "train loss: 61.8938474255607, train metric: 10.767722129821777\n",
      "val loss: 59.83299066776113, val metric: 10.594985961914062\n",
      "----------\n",
      "Epoch 17/49, current lr=0.005\n",
      "train loss: 62.12329697644621, train metric: 10.787517547607422\n",
      "val loss: 60.9466370012183, val metric: 10.704020500183105\n",
      "----------\n",
      "Epoch 18/49, current lr=0.005\n",
      "train loss: 59.600157640699734, train metric: 10.56640625\n",
      "val loss: 61.37188106937261, val metric: 10.70626163482666\n",
      "----------\n",
      "Epoch 19/49, current lr=0.005\n",
      "Copied best model weights!\n",
      "train loss: 59.7597305668857, train metric: 10.583770751953125\n",
      "val loss: 56.84219904095715, val metric: 10.297417640686035\n",
      "----------\n",
      "Epoch 20/49, current lr=0.005\n",
      "train loss: 59.592193556426466, train metric: 10.563342094421387\n",
      "val loss: 58.46983145335998, val metric: 10.473469734191895\n",
      "----------\n",
      "Epoch 21/49, current lr=0.005\n",
      "train loss: 60.453685639445624, train metric: 10.641898155212402\n",
      "val loss: 59.07691118967441, val metric: 10.493277549743652\n",
      "----------\n",
      "Epoch 22/49, current lr=0.005\n",
      "train loss: 60.70819791712963, train metric: 10.65207290649414\n",
      "val loss: 58.57582264426818, val metric: 10.468791007995605\n",
      "----------\n",
      "Epoch 23/49, current lr=0.005\n",
      "Copied best model weights!\n",
      "train loss: 58.519479421725, train metric: 10.46688175201416\n",
      "val loss: 56.13007584250618, val metric: 10.25198745727539\n",
      "----------\n",
      "Epoch 24/49, current lr=0.005\n",
      "train loss: 59.0630854889639, train metric: 10.509613990783691\n",
      "val loss: 79.11970913234835, val metric: 12.22780704498291\n",
      "----------\n",
      "Epoch 25/49, current lr=0.005\n",
      "train loss: 60.68996373174196, train metric: 10.662761688232422\n",
      "val loss: 57.56288316582123, val metric: 10.36905574798584\n",
      "----------\n",
      "Epoch 26/49, current lr=0.005\n",
      "train loss: 59.18828865821819, train metric: 10.524063110351562\n",
      "val loss: 65.32211911675343, val metric: 11.124424934387207\n",
      "----------\n",
      "Epoch 27/49, current lr=0.005\n",
      "train loss: 60.50108752500387, train metric: 10.647046089172363\n",
      "val loss: 58.12606128330273, val metric: 10.42765998840332\n",
      "----------\n",
      "Epoch 28/49, current lr=0.005\n",
      "train loss: 58.80944774941613, train metric: 10.48543643951416\n",
      "val loss: 56.534549854129246, val metric: 10.29694652557373\n",
      "----------\n",
      "Epoch 29/49, current lr=0.005\n",
      "Loading best model weights!\n",
      "train loss: 59.810863875391476, train metric: 10.596837997436523\n",
      "val loss: 72.98523188314958, val metric: 11.837540626525879\n",
      "----------\n",
      "Epoch 30/49, current lr=0.0025\n",
      "Copied best model weights!\n",
      "train loss: 55.2776768246791, train metric: 10.150538444519043\n",
      "val loss: 54.971863214763374, val metric: 10.12881851196289\n",
      "----------\n",
      "Epoch 31/49, current lr=0.0025\n",
      "Copied best model weights!\n",
      "train loss: 54.36228831165152, train metric: 10.060208320617676\n",
      "val loss: 54.064637317529076, val metric: 10.040745735168457\n",
      "----------\n",
      "Epoch 32/49, current lr=0.0025\n",
      "Copied best model weights!\n",
      "train loss: 54.519209655561944, train metric: 10.073974609375\n",
      "val loss: 53.90412201782736, val metric: 10.019854545593262\n",
      "----------\n",
      "Epoch 33/49, current lr=0.0025\n",
      "train loss: 54.58869273620948, train metric: 10.085027694702148\n",
      "val loss: 57.76314116986545, val metric: 10.412473678588867\n",
      "----------\n",
      "Epoch 34/49, current lr=0.0025\n",
      "train loss: 54.74140783319449, train metric: 10.098586082458496\n",
      "val loss: 54.08246153425142, val metric: 10.047964096069336\n",
      "----------\n",
      "Epoch 35/49, current lr=0.0025\n",
      "train loss: 53.564892785507546, train metric: 9.987334251403809\n",
      "val loss: 54.35270074798234, val metric: 10.070903778076172\n",
      "----------\n",
      "Epoch 36/49, current lr=0.0025\n",
      "train loss: 53.8509721494375, train metric: 10.01448917388916\n",
      "val loss: 55.89873675864939, val metric: 10.23972225189209\n",
      "----------\n",
      "Epoch 37/49, current lr=0.0025\n",
      "train loss: 53.84852355638347, train metric: 10.015801429748535\n",
      "val loss: 57.34839414165253, val metric: 10.38524055480957\n",
      "----------\n",
      "Epoch 38/49, current lr=0.0025\n",
      "Loading best model weights!\n",
      "train loss: 53.58448908204152, train metric: 9.989859580993652\n",
      "val loss: 55.583305589756264, val metric: 10.187939643859863\n",
      "----------\n",
      "Epoch 39/49, current lr=0.00125\n",
      "Copied best model weights!\n",
      "train loss: 52.05572307306038, train metric: 9.830604553222656\n",
      "val loss: 52.70069272415325, val metric: 9.906908988952637\n",
      "----------\n",
      "Epoch 40/49, current lr=0.00125\n",
      "train loss: 51.91350757808162, train metric: 9.814643859863281\n",
      "val loss: 56.67260388809676, val metric: 10.314738273620605\n",
      "----------\n",
      "Epoch 41/49, current lr=0.00125\n",
      "train loss: 51.59895885543633, train metric: 9.785738945007324\n",
      "val loss: 53.39838448452908, val metric: 9.990220069885254\n",
      "----------\n",
      "Epoch 42/49, current lr=0.00125\n",
      "train loss: 51.454833301344415, train metric: 9.77287483215332\n",
      "val loss: 55.630145583180116, val metric: 10.180156707763672\n",
      "----------\n",
      "Epoch 43/49, current lr=0.00125\n",
      "Copied best model weights!\n",
      "train loss: 51.303214309340404, train metric: 9.759564399719238\n",
      "val loss: 52.0825749876136, val metric: 9.859814643859863\n",
      "----------\n",
      "Epoch 44/49, current lr=0.00125\n",
      "train loss: 51.1444105490782, train metric: 9.742620468139648\n",
      "val loss: 54.46406371383648, val metric: 10.097357749938965\n",
      "----------\n",
      "Epoch 45/49, current lr=0.00125\n",
      "train loss: 51.31695720905675, train metric: 9.7607421875\n",
      "val loss: 52.47901939436314, val metric: 9.899873733520508\n",
      "----------\n",
      "Epoch 46/49, current lr=0.00125\n",
      "Copied best model weights!\n",
      "train loss: 52.133746194720565, train metric: 9.83266830444336\n",
      "val loss: 51.66885560993184, val metric: 9.801132202148438\n",
      "----------\n",
      "Epoch 47/49, current lr=0.00125\n",
      "train loss: 51.387446786162265, train metric: 9.759641647338867\n",
      "val loss: 53.34659495908879, val metric: 9.964739799499512\n",
      "----------\n",
      "Epoch 48/49, current lr=0.00125\n",
      "train loss: 51.64887672719218, train metric: 9.787498474121094\n",
      "val loss: 52.90485762860287, val metric: 9.932134628295898\n",
      "----------\n",
      "Epoch 49/49, current lr=0.00125\n",
      "train loss: 51.1068808862396, train metric: 9.734272003173828\n",
      "val loss: 53.243917276305886, val metric: 9.973050117492676\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model, loss_history = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6b9a35d-a243-45e0-9162-bde3d83a7ac6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 106.35827088391692, train metric: 14.203215599060059\n",
      "val loss: 83.0602885940312, val metric: 12.543384552001953\n",
      "----------\n",
      "Epoch 1/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 81.36989630321017, train metric: 12.430781364440918\n",
      "val loss: 78.12552666028103, val metric: 12.117409706115723\n",
      "----------\n",
      "Epoch 2/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 84.84777623269326, train metric: 12.67819595336914\n",
      "val loss: 75.39499320023317, val metric: 11.87547492980957\n",
      "----------\n",
      "Epoch 3/49, current lr=0.01\n",
      "train loss: 90.1690798695248, train metric: 13.12238883972168\n",
      "val loss: 78.60841151455993, val metric: 12.202325820922852\n",
      "----------\n",
      "Epoch 4/49, current lr=0.01\n",
      "train loss: 82.53429274808737, train metric: 12.516764640808105\n",
      "val loss: 75.96803249548391, val metric: 11.920330047607422\n",
      "----------\n",
      "Epoch 5/49, current lr=0.01\n",
      "train loss: 82.15164404795354, train metric: 12.460271835327148\n",
      "val loss: 80.17621149858671, val metric: 12.367795944213867\n",
      "----------\n",
      "Epoch 6/49, current lr=0.01\n",
      "train loss: 82.47077212868783, train metric: 12.469754219055176\n",
      "val loss: 78.93371883707134, val metric: 12.074493408203125\n",
      "----------\n",
      "Epoch 7/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 82.88614432127993, train metric: 12.493256568908691\n",
      "val loss: 73.88637689948528, val metric: 11.676748275756836\n",
      "----------\n",
      "Epoch 8/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 77.56819080914048, train metric: 12.111006736755371\n",
      "val loss: 69.44069443471828, val metric: 11.388557434082031\n",
      "----------\n",
      "Epoch 9/49, current lr=0.01\n",
      "train loss: 80.23058605336786, train metric: 12.285501480102539\n",
      "val loss: 85.42761078679749, val metric: 12.635826110839844\n",
      "----------\n",
      "Epoch 10/49, current lr=0.01\n",
      "train loss: 78.81369418669817, train metric: 12.194109916687012\n",
      "val loss: 75.43527964742009, val metric: 11.903989791870117\n",
      "----------\n",
      "Epoch 11/49, current lr=0.01\n",
      "train loss: 77.8243943012266, train metric: 12.135766983032227\n",
      "val loss: 69.92777303554446, val metric: 11.447216033935547\n",
      "----------\n",
      "Epoch 12/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 74.21006500643685, train metric: 11.860349655151367\n",
      "val loss: 64.40786505874865, val metric: 11.010930061340332\n",
      "----------\n",
      "Epoch 13/49, current lr=0.01\n",
      "train loss: 76.48850593994977, train metric: 12.038780212402344\n",
      "val loss: 69.54505648217098, val metric: 11.473793983459473\n",
      "----------\n",
      "Epoch 14/49, current lr=0.01\n",
      "train loss: 75.49591888608481, train metric: 11.96853256225586\n",
      "val loss: 69.22167367205243, val metric: 11.44239616394043\n",
      "----------\n",
      "Epoch 15/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 73.47878556762848, train metric: 11.79100513458252\n",
      "val loss: 64.06099379850248, val metric: 10.971026420593262\n",
      "----------\n",
      "Epoch 16/49, current lr=0.01\n",
      "train loss: 75.42828612529726, train metric: 11.932860374450684\n",
      "val loss: 68.96065345579655, val metric: 11.350382804870605\n",
      "----------\n",
      "Epoch 17/49, current lr=0.01\n",
      "train loss: 74.78435883129625, train metric: 11.859479904174805\n",
      "val loss: 88.58692697217357, val metric: 13.004451751708984\n",
      "----------\n",
      "Epoch 18/49, current lr=0.01\n",
      "train loss: 75.00735550069452, train metric: 11.892266273498535\n",
      "val loss: 68.11598496970855, val metric: 11.28759479522705\n",
      "----------\n",
      "Epoch 19/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 73.75484673507195, train metric: 11.790677070617676\n",
      "val loss: 63.686952943309706, val metric: 10.90255069732666\n",
      "----------\n",
      "Epoch 20/49, current lr=0.01\n",
      "train loss: 71.71956500103349, train metric: 11.641901969909668\n",
      "val loss: 72.44175360048837, val metric: 11.703795433044434\n",
      "----------\n",
      "Epoch 21/49, current lr=0.01\n",
      "train loss: 72.2233739058573, train metric: 11.678359031677246\n",
      "val loss: 65.33311774254558, val metric: 11.077425956726074\n",
      "----------\n",
      "Epoch 22/49, current lr=0.01\n",
      "Copied best model weights!\n",
      "train loss: 70.34269581316713, train metric: 11.519702911376953\n",
      "val loss: 63.56965208850101, val metric: 10.94793701171875\n",
      "----------\n",
      "Epoch 23/49, current lr=0.01\n",
      "train loss: 73.67583439653353, train metric: 11.802330017089844\n",
      "val loss: 65.85146984498836, val metric: 11.131159782409668\n",
      "----------\n",
      "Epoch 24/49, current lr=0.01\n",
      "train loss: 72.90384605483818, train metric: 11.754714012145996\n",
      "val loss: 84.72885965724385, val metric: 12.659034729003906\n",
      "----------\n",
      "Epoch 25/49, current lr=0.01\n",
      "train loss: 71.07542404212856, train metric: 11.609724998474121\n",
      "val loss: 68.44110434586138, val metric: 11.284801483154297\n",
      "----------\n",
      "Epoch 26/49, current lr=0.01\n",
      "train loss: 65.79067710533998, train metric: 11.128376960754395\n",
      "val loss: 60.48287169923214, val metric: 10.646709442138672\n",
      "----------\n",
      "Epoch 32/49, current lr=0.005\n",
      "train loss: 66.18432704896998, train metric: 11.165155410766602\n",
      "val loss: 60.58043241726731, val metric: 10.683544158935547\n",
      "----------\n",
      "Epoch 33/49, current lr=0.005\n",
      "Copied best model weights!\n",
      "train loss: 65.65754545073854, train metric: 11.10616397857666\n",
      "val loss: 58.76118310886974, val metric: 10.4874906539917\n",
      "----------\n",
      "Epoch 34/49, current lr=0.005\n",
      "train loss: 64.44279003476264, train metric: 11.023726463317871\n",
      "val loss: 61.12775913505773, val metric: 10.758345603942871\n",
      "----------\n",
      "Epoch 35/49, current lr=0.005\n",
      "train loss: 64.69209466491851, train metric: 11.038007736206055\n",
      "val loss: 63.40068846906816, val metric: 10.957311630249023\n",
      "----------\n",
      "Epoch 36/49, current lr=0.005\n",
      "Copied best model weights!\n",
      "train loss: 64.33582089298086, train metric: 11.00076675415039\n",
      "val loss: 58.09017486210863, val metric: 10.443117141723633\n",
      "----------\n",
      "Epoch 37/49, current lr=0.005\n",
      "train loss: 64.1524102413149, train metric: 10.984846115112305\n",
      "val loss: 58.24515096816123, val metric: 10.452070236206055\n",
      "----------\n",
      "Epoch 38/49, current lr=0.005\n",
      "train loss: 63.84061903204407, train metric: 10.955106735229492\n",
      "val loss: 58.392187157048156, val metric: 10.476679801940918\n",
      "----------\n",
      "Epoch 39/49, current lr=0.005\n",
      "train loss: 64.9783653463806, train metric: 11.057394027709961\n",
      "val loss: 61.43573861513254, val metric: 10.752281188964844\n",
      "----------\n",
      "Epoch 40/49, current lr=0.005\n",
      "train loss: 66.08567531233714, train metric: 11.175955772399902\n",
      "val loss: 62.41673383615285, val metric: 10.8541898727417\n",
      "----------\n",
      "Epoch 41/49, current lr=0.005\n",
      "train loss: 66.25553184280966, train metric: 11.186915397644043\n",
      "val loss: 60.16160032262353, val metric: 10.647445678710938\n",
      "----------\n",
      "Epoch 42/49, current lr=0.005\n",
      "Loading best model weights!\n",
      "train loss: 64.8273679590582, train metric: 11.053075790405273\n",
      "val loss: 66.77760704304922, val metric: 11.218555450439453\n",
      "----------\n",
      "Epoch 43/49, current lr=0.0025\n",
      "train loss: 68.90015036946818, train metric: 11.344573020935059\n",
      "val loss: 60.673146421194254, val metric: 10.670353889465332\n",
      "----------\n",
      "Epoch 44/49, current lr=0.0025\n",
      "Copied best model weights!\n",
      "train loss: 62.868034806096944, train metric: 10.864934921264648\n",
      "val loss: 57.859203807970616, val metric: 10.415996551513672\n",
      "----------\n",
      "Epoch 45/49, current lr=0.0025\n",
      "Copied best model weights!\n",
      "train loss: 61.89235162140425, train metric: 10.78962230682373\n",
      "val loss: 56.82319509153858, val metric: 10.326020240783691\n",
      "----------\n",
      "Epoch 46/49, current lr=0.0025\n",
      "train loss: 60.71736314921011, train metric: 10.685863494873047\n",
      "val loss: 57.236214464901394, val metric: 10.381237983703613\n",
      "----------\n",
      "Epoch 47/49, current lr=0.0025\n",
      "train loss: 60.76791458177448, train metric: 10.687967300415039\n",
      "val loss: 57.83838052542779, val metric: 10.422680854797363\n",
      "----------\n",
      "Epoch 48/49, current lr=0.0025\n",
      "Copied best model weights!\n",
      "train loss: 60.797894284255484, train metric: 10.684582710266113\n",
      "val loss: 56.4643518585304, val metric: 10.274271011352539\n",
      "----------\n",
      "Epoch 49/49, current lr=0.0025\n",
      "train loss: 60.361783646467025, train metric: 10.650651931762695\n",
      "val loss: 56.47019780172549, val metric: 10.286766052246094\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model, loss_history = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "526473b9-72ba-4b06-9ce3-b854f15edd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.sin(0.1 * np.arange(200)) + np.random.randn(200) * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b842c290-af7d-4e2a-b94c-b4f537ad60a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9edfe8eb-98ef-40ac-93c6-e3f240765102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 10\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "data = torch.FloatTensor(data).view(-1, 1)\n",
    "\n",
    "# Create sequences for input and target\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        target = data[i+seq_length:i+seq_length+1]\n",
    "        sequences.append((seq, target))\n",
    "    return sequences\n",
    "\n",
    "sequences = create_sequences(data, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f7a43-919f-4fa6-92fd-d49402d158b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpus",
   "language": "python",
   "name": "pytorch_gpus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
